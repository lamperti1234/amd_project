{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AMD project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3a07lww5JdF"
      },
      "source": [
        "# README"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_9ZcBJA5CmC"
      },
      "source": [
        "# insert here your kaggle credentials\n",
        "KAGGLE_USERNAME = ''\n",
        "KAGGLE_KEY = ''\n",
        "\n",
        "# if you want to visualize frequent itemsets\n",
        "# N.B. execution time will increase a lot because a dataframe will be created\n",
        "# with DUMP=False only the counts of frequent itemsets will be shown\n",
        "DUMP = False\n",
        "# if you want to save frequent itemsets on file\n",
        "SAVE = False"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P935p6Bq0QUI"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHEcWX8M0oCM",
        "outputId": "2e0c59f2-330a-41d2-fef6-057e7707dbca"
      },
      "source": [
        "!pip install kaggle==1.5.12\n",
        "!pip install pyspark==3.1.2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle==1.5.12 in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.12) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.12) (2021.5.30)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.12) (5.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.12) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.12) (2.8.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.12) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.12) (4.41.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle==1.5.12) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle==1.5.12) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle==1.5.12) (3.0.4)\n",
            "Collecting pyspark==3.1.2\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 63 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 22.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=e78030fc721ed7e7d54369d399907adb9911fee2fce5b300b390206a148fa329\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6ML6W5rtiy_"
      },
      "source": [
        "import csv\n",
        "import logging\n",
        "import os\n",
        "import psutil\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from itertools import combinations\n",
        "from pathlib import Path\n",
        "from pyspark import AccumulatorParam, RDD, Broadcast, Accumulator\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from pyspark.sql import Window\n",
        "from types import FunctionType\n",
        "from typing import Any, Union, Callable, Dict, Optional, Iterator, Iterable, List, Tuple, Set"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCXrB-d8x8U1"
      },
      "source": [
        "def delete_path(path: Union[str, Path]) -> None:\n",
        "    \"\"\"\n",
        "    Remove folder and all subfolders.\n",
        "    \n",
        "    :param path: path to be removed\n",
        "    \"\"\"\n",
        "    shutil.rmtree(path, ignore_errors=True)\n",
        "\n",
        "\n",
        "def get_path(base_path: Union[str, Path], *args: Any, create: bool = False, delete: bool = False) -> Path:\n",
        "    \"\"\"\n",
        "    Join path with default separator. Optionally it can create and/or clean the specified path if it is a folder.\n",
        "\n",
        "    :param base_path: base path to use to new path\n",
        "    :param args: eventually subfolders\n",
        "    :param create: create the folder if not exists\n",
        "    :param delete: clear folder if it already exists\n",
        "    :return: the required path\n",
        "    \"\"\"\n",
        "    path = Path(base_path, *[str(arg) for arg in args if arg])\n",
        "    if delete:\n",
        "        delete_path(path)\n",
        "    if create:\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "\n",
        "def get_filename(path: Union[str, Path]) -> str:\n",
        "    \"\"\"\n",
        "    Get filename or last folder from a path.\n",
        "\n",
        "    :param path: path of a file\n",
        "    :return: the filename or last folder\n",
        "    \"\"\"\n",
        "    return Path(path).stem\n",
        "\n",
        "\n",
        "def set_env_variables(**kwargs: Any) -> None:\n",
        "    \"\"\"\n",
        "    Set environment variables.\n",
        "\n",
        "    :param kwargs: dictionary of key-value environment variables\n",
        "    \"\"\"\n",
        "    for key, value in kwargs.items():\n",
        "        os.environ[key] = str(value)\n",
        "\n",
        "\n",
        "def is_empty(path: Union[str, Path]) -> bool:\n",
        "    \"\"\"\n",
        "    Check if a folder is empty.\n",
        "\n",
        "    :param path: path of a folder\n",
        "    :return: true if path does not exists of if it has no files\n",
        "    \"\"\"\n",
        "    path = Path(path)\n",
        "\n",
        "    return not (path.exists() and bool(os.listdir(path)))\n",
        "\n",
        "\n",
        "def timer(func: FunctionType) -> Callable:\n",
        "    \"\"\"\n",
        "    Calculating execution time for a specific function.\n",
        "    \n",
        "    :param func: function to be executed\n",
        "    :return: wrapper for the function\n",
        "    \"\"\"\n",
        "    def wrapper(*args, **kwargs) -> Any:\n",
        "        start = datetime.now()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = datetime.now()\n",
        "        print(f'Execution time for {func.__name__ }: {end - start}')\n",
        "\n",
        "        return result\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def memory_used(func: FunctionType) -> Callable:\n",
        "    \"\"\"\n",
        "    Calculating memory usage for a specific function.\n",
        "    \n",
        "    :param func: function to be executed\n",
        "    :return: wrapper for the function\n",
        "    \"\"\"\n",
        "    def wrapper(*args, **kwargs) -> Any:\n",
        "        start_mem = psutil.virtual_memory().used\n",
        "        result = func(*args, **kwargs)\n",
        "        end_mem = psutil.virtual_memory().used\n",
        "        print(f'Memory used for {func.__name__ }: {end_mem - start_mem}')\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def read_csvfile(path: Union[str, Path]) -> Iterator[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Read csv file as list of fields skipping header.\n",
        "\n",
        "    :param path: path to csv file\n",
        "    :param header: if header is present\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    logging.info(f'Reading csv path {path}')\n",
        "    with open(path) as file:\n",
        "        reader = csv.reader(file)\n",
        "        for row in reader:\n",
        "            yield row\n",
        "\n",
        "\n",
        "def save_csvfile(data: Iterator, path: Union[str, Path]) -> None:\n",
        "    \"\"\"\n",
        "    Save an iterator as csv file.\n",
        "\n",
        "    :param data: dict to save to file\n",
        "    :param path: path where save csv file\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    with open(path, 'w') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerows(data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkqzzv7vqu7y"
      },
      "source": [
        "def get_spark() -> SparkSession:\n",
        "    \"\"\"\n",
        "    Create a SparkSession with predefined values.\n",
        "\n",
        "    :return: a SparkSession object\n",
        "    \"\"\"\n",
        "    return (SparkSession.builder\n",
        "            .appName('amd')\n",
        "            .getOrCreate())\n",
        "    \n",
        "\n",
        "def check_empty(df: DataFrame) -> bool:\n",
        "    \"\"\"\n",
        "    Check if a dataframe is empty.\n",
        "\n",
        "    :param df: dataframe to be checked\n",
        "    :return: true if dataframe has 0 rows, false otherwise\n",
        "    \"\"\"\n",
        "    return df.take(1).count == 0\n",
        "\n",
        "\n",
        "def read_csv_rdd(path: Union[Path, str], sep: str = ',') -> Optional[RDD]:\n",
        "    \"\"\"\n",
        "    Read a csv file and put it on a RDD. It returns None if no such path exists.\n",
        "\n",
        "    :param path: path to csv file\n",
        "    :param sep: separator used in csv file\n",
        "    :return: dataframe from selected path\n",
        "    \"\"\"\n",
        "    logging.info(f'Reading csv path {path} with sep:{sep}')\n",
        "\n",
        "    path = Path(path)\n",
        "    if path.exists():\n",
        "        return (get_spark()\n",
        "                .sparkContext\n",
        "                .textFile(str(path))\n",
        "                .map(lambda row: row.split(sep))\n",
        "                )\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def read_csv_df(path: Union[Path, str], header: bool = True, sep: str = ',') -> Optional[DataFrame]:\n",
        "    \"\"\"\n",
        "    Read a csv file and put it on a DataFrame. It will escape quote to allow reading multiple lines column.\n",
        "    It returns None if no such path exists.\n",
        "\n",
        "    :param path: path to csv file\n",
        "    :param header: if file has a first row with column names\n",
        "    :param sep: separator used in csv file\n",
        "    :return: dataframe from selected path\n",
        "    \"\"\"\n",
        "    logging.info(f'Reading csv path {path} with header:{header} and sep:{sep}')\n",
        "\n",
        "    path = Path(path)\n",
        "    if path.exists():\n",
        "        return (get_spark()\n",
        "                .read\n",
        "                .option('header', header)\n",
        "                .option('multiLine', True)\n",
        "                .option(\"escape\", \"\\\"\")\n",
        "                .option('sep', sep)\n",
        "                .csv(str(path)))\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "@timer\n",
        "def save_csv_df(df: DataFrame, path: Union[Path, str], header: bool = True) -> None:\n",
        "    \"\"\"\n",
        "    Save a DataFrame in a csv file.\n",
        "    \n",
        "    :param df: dataframe to be saved\n",
        "    :param path: path where to save the file\n",
        "    :param header: if csv file will have an header\n",
        "    :return: \n",
        "    \"\"\"\n",
        "    logging.info(f'Saving csv path {path}')\n",
        "    \n",
        "    df.write.csv(str(path), header=header)\n",
        "\n",
        "\n",
        "def read_parquet(path: Union[Path, str]) -> Optional[DataFrame]:\n",
        "    \"\"\"\n",
        "    Read a parquet file and put in a DataFrame. It returns None if no such path exists.\n",
        "    \n",
        "    :param path: path to parquet file\n",
        "    :return: dataframe from selected path\n",
        "    \"\"\"\n",
        "    logging.info(f'Reading parquet path {path}')\n",
        "\n",
        "    path = Path(path)\n",
        "    if path.exists():\n",
        "        return get_spark().read.parquet(str(path))\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "@timer\n",
        "def save_parquet(df: DataFrame, path: Union[Path, str]) -> None:\n",
        "    \"\"\"\n",
        "    Save a DataFrame in a parquet file.\n",
        "    \n",
        "    :param df: dataframe to be saved\n",
        "    :param path: path where to save the file\n",
        "    :return: \n",
        "    \"\"\"\n",
        "    logging.info(f'Saving parquet path {path}')\n",
        "    \n",
        "    df.write.parquet(str(path))\n",
        "\n",
        "\n",
        "\n",
        "class DictParam(AccumulatorParam):\n",
        "    \"\"\"\n",
        "    It allows to have a dict accumulator.\n",
        "    \"\"\"\n",
        "    def zero(self, value: Dict = None) -> Dict:\n",
        "        return {}\n",
        "\n",
        "    def addInPlace(self, value1: Dict, value2: Dict) -> Dict:\n",
        "        value1.update(value2)\n",
        "\n",
        "        return value1\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK4X-fikzrNz"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAZ1DT9xx3Ms"
      },
      "source": [
        "# directories\n",
        "ROOT_DIR = Path('.').parent.absolute()\n",
        "DATASETS = get_path(ROOT_DIR, 'datasets', create=True)\n",
        "RESULTS = get_path(ROOT_DIR, 'results', create=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18SWT_z85UkO"
      },
      "source": [
        "# Kaggle\n",
        "set_env_variables(KAGGLE_USERNAME=KAGGLE_USERNAME, KAGGLE_KEY=KAGGLE_KEY)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdK0-4cB0F8i"
      },
      "source": [
        "# dataset\n",
        "DATASET = 'ashirwadsangwan/imdb-dataset'\n",
        "APRIORI_THRESHOLD = 30\n",
        "SON_CHUNKS = 5\n",
        "TOIVONEN_MAX_ITERATIONS = 5\n",
        "TOIVONEN_SIZE_SAMPLE = 50000\n",
        "TOIVONEN_THRESHOLD_ADJUST = 0.8\n",
        "DATASET_PATH = get_path(DATASETS, DATASET)\n",
        "RAW_PATH = get_path(DATASET_PATH, 'raw')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fzeizVnKC1d"
      },
      "source": [
        "# loggging\n",
        "LOG_FORMAT = '[%(levelname)s %(asctime)-15s]:%(name)s: %(message)s'\n",
        "LOG_LEVEL = logging.INFO\n",
        "\n",
        "logging.basicConfig(format=LOG_FORMAT, level=LOG_LEVEL)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OraR_SfVyjgP"
      },
      "source": [
        "# Download dataset from Kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs8RDS1GyMEu"
      },
      "source": [
        "@timer\n",
        "def download_dataset(name: str, force: bool = False) -> None:\n",
        "    \"\"\"\n",
        "    Download the specified dataset if not already download.\n",
        "@timer\n",
        "    :param name: name of the dataset\n",
        "    :param force: force downlaod if dataset already exists\n",
        "    \"\"\"\n",
        "    path = get_path(DATASETS, name, create=True, delete=force)\n",
        "    if not is_empty(path):\n",
        "        logging.info(f'Dataset {name} already downloaded!')\n",
        "        return\n",
        "\n",
        "    # require authentication before importing\n",
        "    from kaggle import api\n",
        "\n",
        "    api.dataset_download_cli(dataset=name, path=path, unzip=True)\n",
        "    logging.info(f'Dataset {name} downloaded!')\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8iev3EYzgBp",
        "outputId": "e16fbb79-1275-4c6d-a26f-4316a167f0bc"
      },
      "source": [
        "download_dataset(DATASET)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0.00/1.44G [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading imdb-dataset.zip to /content/datasets/ashirwadsangwan/imdb-dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.44G/1.44G [00:14<00:00, 108MB/s] \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:03:19,796]:root: Dataset ashirwadsangwan/imdb-dataset downloaded!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for download_dataset: 0:02:09.299070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpWZInEh9rHG"
      },
      "source": [
        "# Data extraction and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY1tLVtneoti"
      },
      "source": [
        "@timer\n",
        "def extract_data(force: bool = False) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Extract dataframe using movies, actors and names.\n",
        "\n",
        "    :return: a dataframe\n",
        "    \"\"\"\n",
        "    path = get_path(RAW_PATH, delete=force)\n",
        "    raw_parquet = get_path(path, 'parquet')\n",
        "    raw_csv = get_path(path, 'csv')\n",
        "\n",
        "    if not is_empty(raw_parquet):\n",
        "        logging.info('Reading already extracted data')\n",
        "\n",
        "        return read_parquet(raw_parquet)\n",
        "\n",
        "    if not is_empty(raw_csv):\n",
        "        logging.info('Reading already extracted data')\n",
        "\n",
        "        return read_csv_df(raw_csv)\n",
        "\n",
        "    logging.info('Extracing data from movies, actors and names')\n",
        "\n",
        "    movies = read_csv_df(get_path(DATASET_PATH, 'title.basics.tsv.gz'), sep='\\t').filter('titleType = \"movie\"')\n",
        "    actors = (read_csv_df(get_path(DATASET_PATH, 'title.principals.tsv.gz'), sep='\\t')\n",
        "              .filter('category IN (\"actor\", \"actress\")'))\n",
        "    names = read_csv_df(get_path(DATASET_PATH, 'name.basics.tsv.gz'), sep='\\t')\n",
        "\n",
        "    df = (actors\n",
        "          .join(movies, on='tconst')\n",
        "          .join(names, on='nconst', how='left')\n",
        "          .select('tconst', 'nconst', 'primaryName')\n",
        "          .withColumnRenamed('tconst', 'movie')\n",
        "          .withColumnRenamed('nconst', 'actor1')\n",
        "          .withColumnRenamed('primaryName', 'actor1Name')\n",
        "          ).persist()\n",
        "          \n",
        "    save_parquet(df, raw_parquet)\n",
        "    save_csv_df(df.select('movie', 'actor1')\n",
        "               .groupBy('movie').agg(F.collect_list('actor1').alias('actors'))\n",
        "               .withColumn('actors', F.concat_ws('|', 'actors'))\n",
        "               .coalesce(1), raw_csv, header=False)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElEpcEw8esYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "890d0049-e877-4344-bab3-ce59de1ae948"
      },
      "source": [
        "dataframe = extract_data()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:03:19,836]:root: Extracing data from movies, actors and names\n",
            "[INFO 2021-07-20 11:03:19,838]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/title.basics.tsv.gz with header:True and sep:\t\n",
            "[INFO 2021-07-20 11:03:34,819]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/title.principals.tsv.gz with header:True and sep:\t\n",
            "[INFO 2021-07-20 11:03:35,113]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/name.basics.tsv.gz with header:True and sep:\t\n",
            "[INFO 2021-07-20 11:03:36,414]:root: Saving parquet path /content/datasets/ashirwadsangwan/imdb-dataset/raw/parquet\n",
            "[INFO 2021-07-20 11:05:27,505]:root: Saving csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for save_parquet: 0:01:50.949921\n",
            "Execution time for save_csv_df: 0:00:22.556925\n",
            "Execution time for extract_data: 0:02:30.230504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnVcf3GgWDJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df851b7d-862e-4f24-9aaf-221746e41aef"
      },
      "source": [
        "dataframe.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+---------+--------------+\n",
            "|    movie|   actor1|    actor1Name|\n",
            "+---------+---------+--------------+\n",
            "|tt0083109|nm0000086|Louis de Funès|\n",
            "|tt0062120|nm0000086|Louis de Funès|\n",
            "|tt0057422|nm0000086|Louis de Funès|\n",
            "|tt0058089|nm0000086|Louis de Funès|\n",
            "|tt0067274|nm0000086|Louis de Funès|\n",
            "|tt0079200|nm0000086|Louis de Funès|\n",
            "|tt0048994|nm0000086|Louis de Funès|\n",
            "|tt8768374|nm0000086|Louis de Funès|\n",
            "|tt0066423|nm0000086|Louis de Funès|\n",
            "|tt0063674|nm0000086|Louis de Funès|\n",
            "|tt0056906|nm0000086|Louis de Funès|\n",
            "|tt0078528|nm0000086|Louis de Funès|\n",
            "|tt0046453|nm0000086|Louis de Funès|\n",
            "|tt0061728|nm0000086|Louis de Funès|\n",
            "|tt0057051|nm0000086|Louis de Funès|\n",
            "|tt0069747|nm0000086|Louis de Funès|\n",
            "|tt0054394|nm0000086|Louis de Funès|\n",
            "|tt0059168|nm0000086|Louis de Funès|\n",
            "|tt0057414|nm0000086|Louis de Funès|\n",
            "|tt0052686|nm0000086|Louis de Funès|\n",
            "+---------+---------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhWRr0D6f9Pb"
      },
      "source": [
        "# Data analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wot0e_3dhfzo"
      },
      "source": [
        "Itemset = Tuple[int, ...]\n",
        "CandidateFrequentItemsets = Dict[Itemset, int]\n",
        "FrequentItemsets = CandidateFrequentItemsets\n",
        "BitMap = Dict[int, bool]\n",
        "Transaction = Tuple[str, str]\n",
        "\n",
        "\n",
        "class State:\n",
        "    \"\"\"\n",
        "    State of an algorithm.\n",
        "    At least k (size of the itemset), lk (frequent itemset) and force (if it is needed to recalculate) are present.\n",
        "    \"\"\"\n",
        "    def __init__(self, **state):\n",
        "        self.state = state\n",
        "\n",
        "    def __setitem__(self, key: str, value: Any) -> None:\n",
        "        self.state[key] = value\n",
        "\n",
        "    def __getitem__(self, item: str) -> Any:\n",
        "        return self.state[item]\n",
        "\n",
        "    def __add__(self, other: 'State') -> 'State':\n",
        "        state = {**self.state}\n",
        "        state.update(other.state)\n",
        "\n",
        "        return State(**state)\n",
        "\n",
        "    def update(self, other: 'State') -> None:\n",
        "        self.state.update(other.state)\n",
        "\n",
        "\n",
        "Algorithm = Iterator[State]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaCMtsNShP_s"
      },
      "source": [
        "@timer\n",
        "def dump_frequent_itemsets_stats(df: DataFrame, n: int) -> None:\n",
        "    \"\"\"\n",
        "    Dump some useful information about frequent itemsets.\n",
        "    \n",
        "    :param df: dataframe which contains frequent itemsets\n",
        "    :param n: lenght of frequent itemset\n",
        "    :return: \n",
        "    \"\"\"\n",
        "    logging.info('Calculating frequent itemsets for dataframe')\n",
        "    columns = [column.format(i) for i in range(1, n + 1) for column in ['actor{}', 'actor{}Name']]\n",
        "    # distinct to remove duplicate from eliminating movie column\n",
        "    df = df.select('support', *columns).distinct().orderBy(F.col('support').desc())\n",
        "    df.show()\n",
        "    stats = df.select(\n",
        "        F.count('*').alias('count'),\n",
        "        F.avg('support').alias('avg'),\n",
        "        F.max('support').alias('max'),\n",
        "        F.min('support').alias('min')\n",
        "    )\n",
        "    stats.show()\n",
        "\n",
        "\n",
        "def create_temp_df(lk: CandidateFrequentItemsets, names: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Create a dataframe starting from frequent itemsets.\n",
        "\n",
        "    :param lk: frequent itemsets with count\n",
        "    :param names: names of the actors\n",
        "    :return: dataframe for lk\n",
        "    \"\"\"\n",
        "    df = get_spark().createDataFrame(lk.items(), ['actors', 'support'])\n",
        "    df = df.select('actors.*', 'support')\n",
        "    columns = ['support']\n",
        "\n",
        "    for column in df.columns:\n",
        "        if column.startswith('_'):\n",
        "            number = column[1:]\n",
        "            new_column = f'actor{number}'\n",
        "            name_column = f'{new_column}Name'\n",
        "            df = (df\n",
        "                  .withColumn(column, F.concat(F.lit('nm'), F.lpad(column, 7, '0')))\n",
        "                  .withColumnRenamed(column, new_column)\n",
        "                  )\n",
        "            df = (df\n",
        "                  .join(names, names['nconst'] == F.col(new_column), how='left')\n",
        "                  .withColumnRenamed('PrimaryName', name_column)\n",
        "                  .select(*df.columns, new_column, name_column)\n",
        "                  )\n",
        "\n",
        "            columns.append(new_column)\n",
        "            columns.append(name_column)\n",
        "\n",
        "    return df.select(*columns)\n",
        "\n",
        "\n",
        "def find_csv(path: Union[str, Path]) -> Path:\n",
        "    \"\"\"\n",
        "    Find the csv file created by spark.\n",
        "\n",
        "    :param path: path where to search the csv file\n",
        "    :return: the correct path to csv file\n",
        "    \"\"\"\n",
        "    for f in os.listdir(path):\n",
        "        if f.endswith('.csv'):\n",
        "            return get_path(path, f)\n",
        "\n",
        "\n",
        "def read_frequent_itemsets(path: Union[str, Path]) -> FrequentItemsets:\n",
        "    \"\"\"\n",
        "    Read csv file as frequent itemsets.\n",
        "\n",
        "    :param path: path where frequent itemsets are stored\n",
        "    :return: frequent itemsets in the csv file\n",
        "    \"\"\"\n",
        "    path = get_path(path, 'results.csv')\n",
        "\n",
        "    return {tuple([int(item) for item in row[0].split('|')]): int(row[1]) for row in read_csvfile(path)}\n",
        "\n",
        "\n",
        "def save_frequent_itemsets(itemsets: FrequentItemsets, path: Union[str, Path]) -> None:\n",
        "    \"\"\"\n",
        "    Save frequent itemsets as csv file where keys are the first column and values the second column.\n",
        "\n",
        "    :param itemsets:\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    get_path(path, create=True)\n",
        "    save_csvfile((('|'.join([str(item) for item in key]), value) for key, value in itemsets.items()),\n",
        "                 get_path(path, 'results.csv'))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-_rcsKMhXTi",
        "outputId": "60095ea6-c06c-4a56-d474-d913dc5d9353"
      },
      "source": [
        "file = find_csv(get_path(RAW_PATH, 'csv'))\n",
        "file"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieG5hTl95XXu"
      },
      "source": [
        "## Apriori"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP3bAbr324lx"
      },
      "source": [
        "@timer\n",
        "def get_ck(df: DataFrame, *cols: str) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Extract candidate sets of lenght + 1 with respect to existing itemsets.\n",
        "\n",
        "    :param df: dataframe which contains itemsets\n",
        "    :param cols: columns needed to create new candidate itemsets\n",
        "    :return: a dataframe which contains candidate itemsets\n",
        "    \"\"\"\n",
        "\n",
        "    size = len(cols)\n",
        "\n",
        "    # singleton\n",
        "    if not size:\n",
        "        return df\n",
        "\n",
        "    column = cols[-1]\n",
        "    next_column = f'actor{size + 1}'\n",
        "\n",
        "    small_df = (df.select('movie', f'{column}Name', *cols)\n",
        "                .withColumnRenamed(column, next_column)\n",
        "                .withColumnRenamed(f'{column}Name', f'{next_column}Name'))\n",
        "\n",
        "    # join on movie and all actors needed\n",
        "    join_cond = ['movie'] + list(cols)[:-1]\n",
        "    return df.join(small_df, on=join_cond).filter(f'{column} < {next_column}').persist()\n",
        "\n",
        "\n",
        "@timer\n",
        "def get_lk(state: State) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Extract frequent itemsets from candidate itemsets.\n",
        "\n",
        "    :param state: state of the algorithm\n",
        "    \"\"\"\n",
        "    df = state['df']\n",
        "    threshold = state['threshold']\n",
        "    size = state['k']\n",
        "    cols = [f'actor{i}' for i in range(1, size + 1)]\n",
        "    force = state['force']\n",
        "\n",
        "    path = get_path(RESULTS, f'apriori_{threshold}_{size}', 'parquet', 'apriori', delete=force)\n",
        "\n",
        "    if not is_empty(path):\n",
        "        logging.info('Reading already extracted data')\n",
        "\n",
        "        return read_parquet(path)\n",
        "\n",
        "    logging.info(f'Executing apriori algorithm with {size} items and {threshold} as threshold')\n",
        "\n",
        "    actors_movies = Window.partitionBy(*cols)\n",
        "\n",
        "    df = get_ck(df, *cols[:-1])\n",
        "\n",
        "    df_with_count = df.withColumn('support', F.count('*').over(actors_movies))\n",
        "    df = df_with_count.filter(f'support >= {threshold}').persist()\n",
        "    # dataframe has movie column so without distinct there are duplicates\n",
        "    logging.info(f'Found {df.select(*cols).distinct().count()} frequent itemsets')\n",
        "\n",
        "    if state['save']:\n",
        "        save_parquet(df, path)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def apriori_algorithm(state: State) -> Algorithm:\n",
        "    \"\"\"\n",
        "    Executing apriori algorithm starting from data and a given threshold.\n",
        "\n",
        "    :param state: state of the algorithm:\n",
        "        - df: dataframe which contains candidate itemsets\n",
        "        - threshold: threshold for the apriori algorithm\n",
        "        - force: to force recalculating frequent itemsets\n",
        "    :return: dataframe with frequent itemsets\n",
        "    \"\"\"\n",
        "    state = State(k=1, force=False, save=SAVE) + state\n",
        "    while not check_empty(state['df']):\n",
        "        state['df'] = get_lk(state)\n",
        "        state['k'] += 1\n",
        "\n",
        "        yield state"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTHS7EFR2-Ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52580774-cd11-4be6-e60f-b5ea1855e372"
      },
      "source": [
        "algorithm = apriori_algorithm(State(df=dataframe, threshold=APRIORI_THRESHOLD))\n",
        "\n",
        "singleton = next(algorithm)['df']\n",
        "doubleton = next(algorithm)['df']\n",
        "triple = next(algorithm)['df']\n",
        "quadruple = next(algorithm)['df']\n",
        "quintuple = next(algorithm)['df']"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:05:50,810]:root: Executing apriori algorithm with 1 items and 30 as threshold\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck: 0:00:00.000014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:05:56,994]:root: Found 7453 frequent itemsets\n",
            "[INFO 2021-07-20 11:05:57,129]:root: Executing apriori algorithm with 2 items and 30 as threshold\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk: 0:00:06.190069\n",
            "Execution time for get_ck: 0:00:00.173500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:06:28,806]:root: Found 373 frequent itemsets\n",
            "[INFO 2021-07-20 11:06:28,973]:root: Executing apriori algorithm with 3 items and 30 as threshold\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk: 0:00:31.682864\n",
            "Execution time for get_ck: 0:00:00.294696\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:06:34,865]:root: Found 90 frequent itemsets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk: 0:00:05.900464\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:06:35,133]:root: Executing apriori algorithm with 4 items and 30 as threshold\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck: 0:00:00.311984\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:06:40,858]:root: Found 34 frequent itemsets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk: 0:00:05.738673\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:06:41,248]:root: Executing apriori algorithm with 5 items and 30 as threshold\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck: 0:00:00.491614\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:06:47,607]:root: Found 5 frequent itemsets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk: 0:00:06.369266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5PtYttebnNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc6252e-c247-4267-88e5-78160afa3f1b"
      },
      "source": [
        "if DUMP:\n",
        "    dump_frequent_itemsets_stats(singleton, 1)\n",
        "    dump_frequent_itemsets_stats(doubleton, 2)\n",
        "    dump_frequent_itemsets_stats(triple, 3)\n",
        "    dump_frequent_itemsets_stats(quadruple, 4)\n",
        "    dump_frequent_itemsets_stats(quintuple, 5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:06:47,641]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+------------------+\n",
            "|support|   actor1|        actor1Name|\n",
            "+-------+---------+------------------+\n",
            "|    797|nm0103977|      Brahmanandam|\n",
            "|    585|nm0006982|       Adoor Bhasi|\n",
            "|    565|nm0648803|  Matsunosuke Onoe|\n",
            "|    506|nm0305182|      Eddie Garcia|\n",
            "|    436|nm0623427|        Prem Nazir|\n",
            "|    411|nm0793813|      Sung-il Shin|\n",
            "|    391|nm0246703|      Paquito Diaz|\n",
            "|    387|nm0619107|  Masayoshi Nogami|\n",
            "|    380|nm0007123|         Mammootty|\n",
            "|    356|nm7390393|    Aachi Manorama|\n",
            "|    348|nm0046850|           Bahadur|\n",
            "|    344|nm0482320|          Mohanlal|\n",
            "|    330|nm0149822|Mithun Chakraborty|\n",
            "|    323|nm0304262|   Shivaji Ganesan|\n",
            "|    315|nm0706691|       Sultan Rahi|\n",
            "|    313|nm0619309|            Nagesh|\n",
            "|    311|nm0007106|     Shakti Kapoor|\n",
            "|    303|nm0001000|         Tom Byron|\n",
            "|    303|nm0419653|       Jayabharati|\n",
            "|    303|nm0659250|       Pandharibai|\n",
            "+-------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:06:52,120]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+-----------------+---+---+\n",
            "|count|              avg|max|min|\n",
            "+-----+-----------------+---+---+\n",
            "| 7453|57.35167046826781|797| 30|\n",
            "+-----+-----------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:00:04.478142\n",
            "+-------+---------+--------------------+---------+--------------------+\n",
            "|support|   actor1|          actor1Name|   actor2|          actor2Name|\n",
            "+-------+---------+--------------------+---------+--------------------+\n",
            "|    236|nm0006982|         Adoor Bhasi|nm0623427|          Prem Nazir|\n",
            "|    169|nm0006982|         Adoor Bhasi|nm0046850|             Bahadur|\n",
            "|    162|nm0006982|         Adoor Bhasi|nm0419653|         Jayabharati|\n",
            "|    147|nm0648803|    Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|\n",
            "|    126|nm0648803|    Matsunosuke Onoe|nm2373718|    Kitsuraku Arashi|\n",
            "|    122|nm0006982|         Adoor Bhasi|nm0619779|Thikkurisi Sukuma...|\n",
            "|    113|nm0648803|    Matsunosuke Onoe|nm2077739|   Suminojo Ichikawa|\n",
            "|    113|nm2082516|        Kijaku Ôtani|nm2373718|    Kitsuraku Arashi|\n",
            "|    109|nm0046850|             Bahadur|nm0419653|         Jayabharati|\n",
            "|    103|nm0659173|            Panchito|nm1006879|              Dolphy|\n",
            "|    101|nm2077739|   Suminojo Ichikawa|nm2082516|        Kijaku Ôtani|\n",
            "|    101|nm0619779|Thikkurisi Sukuma...|nm0623427|          Prem Nazir|\n",
            "|     97|nm2077739|   Suminojo Ichikawa|nm2373718|    Kitsuraku Arashi|\n",
            "|     97|nm0648803|    Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|\n",
            "|     96|nm2366585|       Ritoku Arashi|nm2384746|         Hôshô Bandô|\n",
            "|     96|nm0046850|             Bahadur|nm0623427|          Prem Nazir|\n",
            "|     92|nm0006982|         Adoor Bhasi|nm0080246|  Paravoor Bharathan|\n",
            "|     92|nm0419653|         Jayabharati|nm0623427|          Prem Nazir|\n",
            "|     90|nm0006982|         Adoor Bhasi|nm1467390|               Meena|\n",
            "|     90|nm1698868|     Enshô Jitsukawa|nm2366585|       Ritoku Arashi|\n",
            "+-------+---------+--------------------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:06:56,524]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+------------------+---+---+\n",
            "|count|               avg|max|min|\n",
            "+-----+------------------+---+---+\n",
            "|  373|45.501340482573724|236| 30|\n",
            "+-----+------------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:00:04.404519\n",
            "+-------+---------+-------------------+---------+--------------------+---------+------------------+\n",
            "|support|   actor1|         actor1Name|   actor2|          actor2Name|   actor3|        actor3Name|\n",
            "+-------+---------+-------------------+---------+--------------------+---------+------------------+\n",
            "|    112|nm0648803|   Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|    100|nm0648803|   Matsunosuke Onoe|nm2077739|   Suminojo Ichikawa|nm2082516|      Kijaku Ôtani|\n",
            "|     95|nm0648803|   Matsunosuke Onoe|nm2077739|   Suminojo Ichikawa|nm2373718|  Kitsuraku Arashi|\n",
            "|     87|nm2077739|  Suminojo Ichikawa|nm2082516|        Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     80|nm0648803|   Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|nm2082516|      Kijaku Ôtani|\n",
            "|     75|nm0006982|        Adoor Bhasi|nm0046850|             Bahadur|nm0419653|       Jayabharati|\n",
            "|     74|nm0006982|        Adoor Bhasi|nm0619779|Thikkurisi Sukuma...|nm0623427|        Prem Nazir|\n",
            "|     70|nm0648803|   Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|nm2373718|  Kitsuraku Arashi|\n",
            "|     69|nm1698868|    Enshô Jitsukawa|nm2366585|       Ritoku Arashi|nm2384746|       Hôshô Bandô|\n",
            "|     64|nm0648803|   Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|nm2077739| Suminojo Ichikawa|\n",
            "|     63|nm1770187|Sen'nosuke Nakamura|nm2082516|        Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     61|nm0006982|        Adoor Bhasi|nm0419653|         Jayabharati|nm0623427|        Prem Nazir|\n",
            "|     60|nm0006982|        Adoor Bhasi|nm0046850|             Bahadur|nm0623427|        Prem Nazir|\n",
            "|     60|nm2366585|      Ritoku Arashi|nm2367854|        Shôzô Arashi|nm2384746|       Hôshô Bandô|\n",
            "|     58|nm1698868|    Enshô Jitsukawa|nm2366585|       Ritoku Arashi|nm2367854|      Shôzô Arashi|\n",
            "|     58|nm0297793|       Hideo Fujino|nm0945427|     Kaichi Yamamoto|nm2394215|       Takeo Azuma|\n",
            "|     56|nm0648803|   Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|nm2426685|Kakumatsuro Arashi|\n",
            "|     55|nm0648803|   Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|nm2373151|    Chosei Kataoka|\n",
            "|     55|nm1770187|Sen'nosuke Nakamura|nm2077739|   Suminojo Ichikawa|nm2082516|      Kijaku Ôtani|\n",
            "|     54|nm0006982|        Adoor Bhasi|nm0616102|       T.S. Muthaiah|nm0623427|        Prem Nazir|\n",
            "+-------+---------+-------------------+---------+--------------------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:07:00,797]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+-----------------+---+---+\n",
            "|count|              avg|max|min|\n",
            "+-----+-----------------+---+---+\n",
            "|   90|45.44444444444444|112| 30|\n",
            "+-----+-----------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:00:04.272377\n",
            "+-------+---------+-------------------+---------+-------------------+---------+-----------------+---------+------------------+\n",
            "|support|   actor1|         actor1Name|   actor2|         actor2Name|   actor3|       actor3Name|   actor4|        actor4Name|\n",
            "+-------+---------+-------------------+---------+-------------------+---------+-----------------+---------+------------------+\n",
            "|     86|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     62|nm0648803|   Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     54|nm0648803|   Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2077739|Suminojo Ichikawa|nm2082516|      Kijaku Ôtani|\n",
            "|     51|nm1698868|    Enshô Jitsukawa|nm2366585|      Ritoku Arashi|nm2367854|     Shôzô Arashi|nm2384746|       Hôshô Bandô|\n",
            "|     51|nm0648803|   Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2077739|Suminojo Ichikawa|nm2373718|  Kitsuraku Arashi|\n",
            "|     48|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2373151|   Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     46|nm0648803|   Matsunosuke Onoe|nm1283907|      Utae Nakamura|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     45|nm1770187|Sen'nosuke Nakamura|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     45|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373151|    Chosei Kataoka|\n",
            "|     44|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2373151|   Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     42|nm2077739|  Suminojo Ichikawa|nm2082516|       Kijaku Ôtani|nm2373151|   Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     42|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2373718| Kitsuraku Arashi|nm2426685|Kakumatsuro Arashi|\n",
            "|     38|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2687024|     Rihaku Arashi|\n",
            "|     38|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2426685|Kakumatsuro Arashi|\n",
            "|     37|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2373718| Kitsuraku Arashi|nm2687024|     Rihaku Arashi|\n",
            "|     37|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2373718| Kitsuraku Arashi|nm2426685|Kakumatsuro Arashi|\n",
            "|     36|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2369538| Ichitarô Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     36|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2369538|  Ichitarô Kataoka|\n",
            "|     35|nm0297793|       Hideo Fujino|nm0455938| Teinosuke Kinugasa|nm0945427|  Kaichi Yamamoto|nm2394215|       Takeo Azuma|\n",
            "|     34|nm0297793|       Hideo Fujino|nm0945427|    Kaichi Yamamoto|nm1156772|   Unpei Yokoyama|nm2394215|       Takeo Azuma|\n",
            "+-------+---------+-------------------+---------+-------------------+---------+-----------------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:07:05,587]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+------------------+---+---+\n",
            "|count|               avg|max|min|\n",
            "+-----+------------------+---+---+\n",
            "|   34|39.705882352941174| 86| 30|\n",
            "+-----+------------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:00:04.790236\n",
            "+-------+---------+----------------+---------+-------------------+---------+-----------------+---------+----------------+---------+------------------+\n",
            "|support|   actor1|      actor1Name|   actor2|         actor2Name|   actor3|       actor3Name|   actor4|      actor4Name|   actor5|        actor5Name|\n",
            "+-------+---------+----------------+---------+-------------------+---------+-----------------+---------+----------------+---------+------------------+\n",
            "|     44|nm0648803|Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2077739|Suminojo Ichikawa|nm2082516|    Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     42|nm0648803|Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373151|  Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     31|nm0648803|Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|Kitsuraku Arashi|nm2426685|Kakumatsuro Arashi|\n",
            "|     30|nm0648803|Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|Kitsuraku Arashi|nm2687024|     Rihaku Arashi|\n",
            "|     30|nm0648803|Matsunosuke Onoe|nm1283907|      Utae Nakamura|nm2077739|Suminojo Ichikawa|nm2082516|    Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "+-------+---------+----------------+---------+-------------------+---------+-----------------+---------+----------------+---------+------------------+\n",
            "\n",
            "+-----+----+---+---+\n",
            "|count| avg|max|min|\n",
            "+-----+----+---+---+\n",
            "|    5|35.4| 44| 30|\n",
            "+-----+----+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:00:05.320178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0Ghd1OvdPeS"
      },
      "source": [
        "## Apriori in memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH97rzV9Bcat",
        "outputId": "28a46995-ccf5-4807-9da2-e448f4eb6ec0"
      },
      "source": [
        "with open(file) as csvfile:\n",
        "  print(next(csvfile))\n",
        "  print(next(csvfile))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tt0002591,nm0509573|nm0029806\n",
            "\n",
            "tt0003689,nm0585503|nm0694718|nm0101071|nm0910564|nm0527801|nm0399988|nm0728289\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQm6qQrVR-PX"
      },
      "source": [
        "@timer\n",
        "def get_ck_v2(transactions: Iterator[Transaction], k: int,\n",
        "           monotonicity_filter: Callable[[Itemset], int]) -> CandidateFrequentItemsets:\n",
        "    \"\"\"\n",
        "    Scan the file and extract candidate frequent itemsets checking the monotonicity condition.\n",
        "\n",
        "    :param transactions: iterator of transactions\n",
        "    :param k: size of the itemset\n",
        "    :param monotonicity_filter: filter for the monotonicity condition\n",
        "    :return: candidate frequent itemsets\n",
        "    \"\"\"\n",
        "    accumulator = defaultdict(int)\n",
        "    for row in transactions:\n",
        "        raw_actors = [int(actor[2:]) for actor in row[1].split('|')]\n",
        "        for comb in combinations(sorted(raw_actors), k):\n",
        "            if monotonicity_filter(comb):\n",
        "                accumulator[comb] += 1\n",
        "    return accumulator\n",
        "\n",
        "\n",
        "@timer\n",
        "def get_lk_v2(transactions: Iterator[Transaction], state: State) -> FrequentItemsets:\n",
        "    \"\"\"\n",
        "    Extract frequent itemsets checking the support.\n",
        "\n",
        "    :param transactions: iterator of transactions\n",
        "    :param state: state of the algorithm\n",
        "    :return: frequent itemsets\n",
        "    \"\"\"\n",
        "    size = state['k']\n",
        "    old_lk = state['lk']\n",
        "    threshold = state['threshold']\n",
        "    force = state['force']\n",
        "\n",
        "    path = get_path(RESULTS, f'apriori_{threshold}_{size}', 'csv', 'apriori', delete=force)\n",
        "\n",
        "    if not is_empty(path):\n",
        "        logging.info('Reading already extracted data')\n",
        "\n",
        "        return read_frequent_itemsets(path)\n",
        "\n",
        "    logging.info(f'Executing apriori algorithm with {size} items and {threshold} as threshold')\n",
        "\n",
        "    if size == 1:\n",
        "        ck = get_ck_v2(transactions, size, lambda _: True)\n",
        "    else:\n",
        "        ck = get_ck_v2(transactions, size,\n",
        "                    lambda itemset: all([item in old_lk for item in combinations(itemset, size - 1)]))\n",
        "\n",
        "    lk = {item: support\n",
        "          for item, support in ck.items()\n",
        "          if support >= threshold}\n",
        "    logging.info(f'Found {len(lk)} frequent itemsets')\n",
        "\n",
        "    if state['save']:\n",
        "        save_frequent_itemsets(lk, path)\n",
        "\n",
        "    return lk\n",
        "\n",
        "\n",
        "def apriori_algorithm_v2(transactions: Callable[[], Iterator[Transaction]], state: State) -> Algorithm:\n",
        "    \"\"\"\n",
        "    Executing apriori algorithm starting from data and a given threshold.\n",
        "\n",
        "    :param transactions: callable to an iterator of transactions\n",
        "    :param state: state of the algorithm:\n",
        "        - threshold: threshold for the apriori algorithm\n",
        "        - k: size of the itemsets\n",
        "        - lk: frequent itemsets with size k-1\n",
        "        - force: to force recalculating frequent itemsets\n",
        "    :return: dict of frequent itemsets\n",
        "    \"\"\"\n",
        "    state = State(k=1, lk={}, force=False, save=SAVE) + state\n",
        "\n",
        "    while state['k'] == 1 or state['lk']:\n",
        "        state['lk'] = get_lk_v2(transactions(), state)\n",
        "        state['k'] += 1\n",
        "\n",
        "        yield state"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkrflxJzGqxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247efd97-510e-4554-b8cd-2bf4eb773672"
      },
      "source": [
        "algorithm_v2 = apriori_algorithm_v2(lambda: read_csvfile(file), State(threshold=APRIORI_THRESHOLD))\n",
        "\n",
        "singleton_v2 = next(algorithm_v2)['lk']\n",
        "doubleton_v2 = next(algorithm_v2)['lk']\n",
        "triple_v2 = next(algorithm_v2)['lk']\n",
        "quadruple_v2 = next(algorithm_v2)['lk']\n",
        "quintuple_v2 = next(algorithm_v2)['lk']"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:07:11,007]:root: Executing apriori algorithm with 1 items and 30 as threshold\n",
            "[INFO 2021-07-20 11:07:11,010]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n",
            "[INFO 2021-07-20 11:07:13,843]:root: Found 7453 frequent itemsets\n",
            "[INFO 2021-07-20 11:07:13,867]:root: Executing apriori algorithm with 2 items and 30 as threshold\n",
            "[INFO 2021-07-20 11:07:13,870]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v2: 0:00:02.794869\n",
            "Execution time for get_lk_v2: 0:00:02.859134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:07:18,751]:root: Found 373 frequent itemsets\n",
            "[INFO 2021-07-20 11:07:18,764]:root: Executing apriori algorithm with 3 items and 30 as threshold\n",
            "[INFO 2021-07-20 11:07:18,767]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v2: 0:00:04.868524\n",
            "Execution time for get_lk_v2: 0:00:04.896488\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:07:24,127]:root: Found 90 frequent itemsets\n",
            "[INFO 2021-07-20 11:07:24,128]:root: Executing apriori algorithm with 4 items and 30 as threshold\n",
            "[INFO 2021-07-20 11:07:24,136]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v2: 0:00:05.359565\n",
            "Execution time for get_lk_v2: 0:00:05.364041\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:07:29,758]:root: Found 34 frequent itemsets\n",
            "[INFO 2021-07-20 11:07:29,760]:root: Executing apriori algorithm with 5 items and 30 as threshold\n",
            "[INFO 2021-07-20 11:07:29,766]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v2: 0:00:05.622153\n",
            "Execution time for get_lk_v2: 0:00:05.631350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:07:34,710]:root: Found 5 frequent itemsets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v2: 0:00:04.943719\n",
            "Execution time for get_lk_v2: 0:00:04.951622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m2hm7xhMhLH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4fe6d28-c24e-4d21-bf1e-d793ac73ef11"
      },
      "source": [
        "if DUMP:\n",
        "    names = read_csv_df(get_path(DATASET_PATH, 'name.basics.tsv.gz'), sep='\\t')\n",
        "    dump_frequent_itemsets_stats(create_temp_df(singleton_v2, names), 1)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(doubleton_v2, names), 2)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(triple_v2, names), 3)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(quadruple_v2, names), 4)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(quintuple_v2, names), 5)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:07:34,729]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/name.basics.tsv.gz with header:True and sep:\t\n",
            "[INFO 2021-07-20 11:07:35,802]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+------------------+\n",
            "|support|   actor1|        actor1Name|\n",
            "+-------+---------+------------------+\n",
            "|    797|nm0103977|      Brahmanandam|\n",
            "|    585|nm0006982|       Adoor Bhasi|\n",
            "|    565|nm0648803|  Matsunosuke Onoe|\n",
            "|    506|nm0305182|      Eddie Garcia|\n",
            "|    436|nm0623427|        Prem Nazir|\n",
            "|    411|nm0793813|      Sung-il Shin|\n",
            "|    391|nm0246703|      Paquito Diaz|\n",
            "|    387|nm0619107|  Masayoshi Nogami|\n",
            "|    380|nm0007123|         Mammootty|\n",
            "|    356|nm7390393|    Aachi Manorama|\n",
            "|    348|nm0046850|           Bahadur|\n",
            "|    344|nm0482320|          Mohanlal|\n",
            "|    330|nm0149822|Mithun Chakraborty|\n",
            "|    323|nm0304262|   Shivaji Ganesan|\n",
            "|    315|nm0706691|       Sultan Rahi|\n",
            "|    313|nm0619309|            Nagesh|\n",
            "|    311|nm0007106|     Shakti Kapoor|\n",
            "|    303|nm0659250|       Pandharibai|\n",
            "|    303|nm0419653|       Jayabharati|\n",
            "|    303|nm0001000|         Tom Byron|\n",
            "+-------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-----------------+---+---+\n",
            "|count|              avg|max|min|\n",
            "+-----+-----------------+---+---+\n",
            "| 7453|57.35167046826781|797| 30|\n",
            "+-----+-----------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:00:51.628048\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:08:27,677]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+--------------------+---------+--------------------+\n",
            "|support|   actor1|          actor1Name|   actor2|          actor2Name|\n",
            "+-------+---------+--------------------+---------+--------------------+\n",
            "|    236|nm0006982|         Adoor Bhasi|nm0623427|          Prem Nazir|\n",
            "|    169|nm0006982|         Adoor Bhasi|nm0046850|             Bahadur|\n",
            "|    162|nm0006982|         Adoor Bhasi|nm0419653|         Jayabharati|\n",
            "|    147|nm0648803|    Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|\n",
            "|    126|nm0648803|    Matsunosuke Onoe|nm2373718|    Kitsuraku Arashi|\n",
            "|    122|nm0006982|         Adoor Bhasi|nm0619779|Thikkurisi Sukuma...|\n",
            "|    113|nm2082516|        Kijaku Ôtani|nm2373718|    Kitsuraku Arashi|\n",
            "|    113|nm0648803|    Matsunosuke Onoe|nm2077739|   Suminojo Ichikawa|\n",
            "|    109|nm0046850|             Bahadur|nm0419653|         Jayabharati|\n",
            "|    103|nm0659173|            Panchito|nm1006879|              Dolphy|\n",
            "|    101|nm0619779|Thikkurisi Sukuma...|nm0623427|          Prem Nazir|\n",
            "|    101|nm2077739|   Suminojo Ichikawa|nm2082516|        Kijaku Ôtani|\n",
            "|     97|nm2077739|   Suminojo Ichikawa|nm2373718|    Kitsuraku Arashi|\n",
            "|     97|nm0648803|    Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|\n",
            "|     96|nm0046850|             Bahadur|nm0623427|          Prem Nazir|\n",
            "|     96|nm2366585|       Ritoku Arashi|nm2384746|         Hôshô Bandô|\n",
            "|     92|nm0006982|         Adoor Bhasi|nm0080246|  Paravoor Bharathan|\n",
            "|     92|nm0419653|         Jayabharati|nm0623427|          Prem Nazir|\n",
            "|     90|nm0006982|         Adoor Bhasi|nm1467390|               Meena|\n",
            "|     90|nm1698868|     Enshô Jitsukawa|nm2366585|       Ritoku Arashi|\n",
            "+-------+---------+--------------------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+------------------+---+---+\n",
            "|count|               avg|max|min|\n",
            "+-----+------------------+---+---+\n",
            "|  373|45.501340482573724|236| 30|\n",
            "+-----+------------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:00:58.642802\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:09:26,629]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+-------------------+---------+--------------------+---------+------------------+\n",
            "|support|   actor1|         actor1Name|   actor2|          actor2Name|   actor3|        actor3Name|\n",
            "+-------+---------+-------------------+---------+--------------------+---------+------------------+\n",
            "|    112|nm0648803|   Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|    100|nm0648803|   Matsunosuke Onoe|nm2077739|   Suminojo Ichikawa|nm2082516|      Kijaku Ôtani|\n",
            "|     95|nm0648803|   Matsunosuke Onoe|nm2077739|   Suminojo Ichikawa|nm2373718|  Kitsuraku Arashi|\n",
            "|     87|nm2077739|  Suminojo Ichikawa|nm2082516|        Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     80|nm0648803|   Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|nm2082516|      Kijaku Ôtani|\n",
            "|     75|nm0006982|        Adoor Bhasi|nm0046850|             Bahadur|nm0419653|       Jayabharati|\n",
            "|     74|nm0006982|        Adoor Bhasi|nm0619779|Thikkurisi Sukuma...|nm0623427|        Prem Nazir|\n",
            "|     70|nm0648803|   Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|nm2373718|  Kitsuraku Arashi|\n",
            "|     69|nm1698868|    Enshô Jitsukawa|nm2366585|       Ritoku Arashi|nm2384746|       Hôshô Bandô|\n",
            "|     64|nm0648803|   Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|nm2077739| Suminojo Ichikawa|\n",
            "|     63|nm1770187|Sen'nosuke Nakamura|nm2082516|        Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     61|nm0006982|        Adoor Bhasi|nm0419653|         Jayabharati|nm0623427|        Prem Nazir|\n",
            "|     60|nm2366585|      Ritoku Arashi|nm2367854|        Shôzô Arashi|nm2384746|       Hôshô Bandô|\n",
            "|     60|nm0006982|        Adoor Bhasi|nm0046850|             Bahadur|nm0623427|        Prem Nazir|\n",
            "|     58|nm1698868|    Enshô Jitsukawa|nm2366585|       Ritoku Arashi|nm2367854|      Shôzô Arashi|\n",
            "|     58|nm0297793|       Hideo Fujino|nm0945427|     Kaichi Yamamoto|nm2394215|       Takeo Azuma|\n",
            "|     56|nm0648803|   Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|nm2426685|Kakumatsuro Arashi|\n",
            "|     55|nm0648803|   Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|nm2373151|    Chosei Kataoka|\n",
            "|     55|nm1770187|Sen'nosuke Nakamura|nm2077739|   Suminojo Ichikawa|nm2082516|      Kijaku Ôtani|\n",
            "|     54|nm0006982|        Adoor Bhasi|nm0616102|       T.S. Muthaiah|nm0623427|        Prem Nazir|\n",
            "+-------+---------+-------------------+---------+--------------------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-----------------+---+---+\n",
            "|count|              avg|max|min|\n",
            "+-----+-----------------+---+---+\n",
            "|   90|45.44444444444444|112| 30|\n",
            "+-----+-----------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:01:05.637717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:10:32,789]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+-------------------+---------+-------------------+---------+-----------------+---------+------------------+\n",
            "|support|   actor1|         actor1Name|   actor2|         actor2Name|   actor3|       actor3Name|   actor4|        actor4Name|\n",
            "+-------+---------+-------------------+---------+-------------------+---------+-----------------+---------+------------------+\n",
            "|     86|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     62|nm0648803|   Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     54|nm0648803|   Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2077739|Suminojo Ichikawa|nm2082516|      Kijaku Ôtani|\n",
            "|     51|nm1698868|    Enshô Jitsukawa|nm2366585|      Ritoku Arashi|nm2367854|     Shôzô Arashi|nm2384746|       Hôshô Bandô|\n",
            "|     51|nm0648803|   Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2077739|Suminojo Ichikawa|nm2373718|  Kitsuraku Arashi|\n",
            "|     48|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2373151|   Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     46|nm0648803|   Matsunosuke Onoe|nm1283907|      Utae Nakamura|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     45|nm1770187|Sen'nosuke Nakamura|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     45|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373151|    Chosei Kataoka|\n",
            "|     44|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2373151|   Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     42|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2373718| Kitsuraku Arashi|nm2426685|Kakumatsuro Arashi|\n",
            "|     42|nm2077739|  Suminojo Ichikawa|nm2082516|       Kijaku Ôtani|nm2373151|   Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     38|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2426685|Kakumatsuro Arashi|\n",
            "|     38|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2687024|     Rihaku Arashi|\n",
            "|     37|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2373718| Kitsuraku Arashi|nm2426685|Kakumatsuro Arashi|\n",
            "|     37|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2373718| Kitsuraku Arashi|nm2687024|     Rihaku Arashi|\n",
            "|     36|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2369538|  Ichitarô Kataoka|\n",
            "|     36|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2369538| Ichitarô Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     35|nm0297793|       Hideo Fujino|nm0455938| Teinosuke Kinugasa|nm0945427|  Kaichi Yamamoto|nm2394215|       Takeo Azuma|\n",
            "|     34|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2414317|  Sentarô Nakamura|\n",
            "+-------+---------+-------------------+---------+-------------------+---------+-----------------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+------------------+---+---+\n",
            "|count|               avg|max|min|\n",
            "+-----+------------------+---+---+\n",
            "|   34|39.705882352941174| 86| 30|\n",
            "+-----+------------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:01:14.456439\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:11:47,681]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+----------------+---------+-------------------+---------+-----------------+---------+----------------+---------+------------------+\n",
            "|support|   actor1|      actor1Name|   actor2|         actor2Name|   actor3|       actor3Name|   actor4|      actor4Name|   actor5|        actor5Name|\n",
            "+-------+---------+----------------+---------+-------------------+---------+-----------------+---------+----------------+---------+------------------+\n",
            "|     44|nm0648803|Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2077739|Suminojo Ichikawa|nm2082516|    Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     42|nm0648803|Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373151|  Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     31|nm0648803|Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|Kitsuraku Arashi|nm2426685|Kakumatsuro Arashi|\n",
            "|     30|nm0648803|Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|Kitsuraku Arashi|nm2687024|     Rihaku Arashi|\n",
            "|     30|nm0648803|Matsunosuke Onoe|nm1283907|      Utae Nakamura|nm2077739|Suminojo Ichikawa|nm2082516|    Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "+-------+---------+----------------+---------+-------------------+---------+-----------------+---------+----------------+---------+------------------+\n",
            "\n",
            "+-----+----+---+---+\n",
            "|count| avg|max|min|\n",
            "+-----+----+---+---+\n",
            "|    5|35.4| 44| 30|\n",
            "+-----+----+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:01:24.660520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWqw6CS3C_kh"
      },
      "source": [
        "## PCY multihash\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPHN3uSZkIZ_"
      },
      "source": [
        "@timer\n",
        "def get_ck_v3(transactions: Iterator[Transaction], monotonicity_filter: Callable[[Itemset], bool],\n",
        "           bitmaps_filter: Callable[[Itemset], bool],\n",
        "           state: State) -> Tuple[CandidateFrequentItemsets, List[BitMap]]:\n",
        "    \"\"\"\n",
        "    Scan the file and extract candidate frequent itemsets checking the monotonicity condition and bitmaps.\n",
        "\n",
        "    :param transactions: iterator of transactions\n",
        "    :param monotonicity_filter: filter for the monotonicity condition\n",
        "    :param bitmaps_filter: filter for the bitmaps\n",
        "    :param state: state of the algorithm\n",
        "    :return: candidate frequent itemsets\n",
        "    \"\"\"\n",
        "    k = state['k']\n",
        "    hash_functions = state['hash_functions']\n",
        "    buckets = state['buckets']\n",
        "    threshold = state['threshold']\n",
        "\n",
        "    accumulator = defaultdict(int)\n",
        "    counters = [defaultdict(int)] * len(hash_functions)\n",
        "    for row in transactions:\n",
        "        raw_actors = [int(actor[2:]) for actor in row[1].split('|')]\n",
        "        for comb in combinations(sorted(raw_actors), k):\n",
        "            if monotonicity_filter(comb) and bitmaps_filter(comb):\n",
        "                accumulator[comb] += 1\n",
        "        for comb in combinations(sorted(raw_actors), k + 1):\n",
        "            for counter, hash_function in zip(counters, hash_functions):\n",
        "                bucket = hash_function(buckets, comb)\n",
        "                counter[bucket] += 1\n",
        "    return accumulator, [\n",
        "        {bucket: count >= threshold for bucket, count in counter.items()}\n",
        "        for counter in counters\n",
        "    ]\n",
        "\n",
        "\n",
        "@timer\n",
        "def get_lk_v3(transactions: Iterator[Transaction], state: State) -> Tuple[FrequentItemsets, List[BitMap]]:\n",
        "    \"\"\"\n",
        "    Extract frequent itemsets checking the support.\n",
        "\n",
        "    :param transactions: iterator of transactions\n",
        "    :param state: state of the algorithm\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    size = state['k']\n",
        "    old_lk = state['lk']\n",
        "    threshold = state['threshold']\n",
        "    bitmaps = state['bitmaps']\n",
        "    old_buckets = state['old_buckets']\n",
        "    hash_functions = state['hash_functions']\n",
        "    force = state['force']\n",
        "\n",
        "    path = get_path(RESULTS, f'apriori_{threshold}_{size}', 'csv', 'pcy', delete=force)\n",
        "\n",
        "    if not is_empty(path):\n",
        "        logging.info('Reading already extracted data')\n",
        "\n",
        "        return read_frequent_itemsets(path), []\n",
        "\n",
        "    logging.info(f'Executing apriori algorithm with {size} items and {threshold} as threshold')\n",
        "\n",
        "    def monotonicity(itemset: Itemset) -> bool:\n",
        "        return size == 1 or all([item in old_lk for item in combinations(itemset, size - 1)])\n",
        "\n",
        "    def check_bitmaps(itemset: Itemset) -> bool:\n",
        "        return all([bitmap[hash_function(old_buckets, itemset)]]\n",
        "                   for bitmap, hash_function in zip(bitmaps, hash_functions))\n",
        "\n",
        "    ck, bitmaps = get_ck_v3(transactions, monotonicity, check_bitmaps, state)\n",
        "    lk = {item: support\n",
        "          for item, support in ck.items()\n",
        "          if support >= threshold}\n",
        "    logging.info(f'Found {len(lk)} frequent itemsets')\n",
        "\n",
        "    if state['save']:\n",
        "        save_frequent_itemsets(lk, path)\n",
        "\n",
        "    return lk, bitmaps\n",
        "\n",
        "\n",
        "def pcy_algorithm(transactions: Callable[[], Iterator[Transaction]], state: State) -> Algorithm:\n",
        "    \"\"\"\n",
        "    Executing apriori algorithm starting from data and a given threshold.\n",
        "\n",
        "    :param transactions: callable to an iterator of transactions\n",
        "    :param state: state of the algorithm:\n",
        "        - threshold: threshold for the apriori algorithm\n",
        "        - k: size of the itemsets\n",
        "        - lk: frequent itemsets with size k-1\n",
        "        - bitmaps: bitmaps previously calculated for frequent itemset with size k+1\n",
        "        - hash_functions: hash function to be applied to candidate frequent itemsets\n",
        "        - old_buckets: number of buckets for the previous bitmaps\n",
        "        - buckets: number of buckets for the current bitmaps\n",
        "        - force: to force recalculating frequent itemsets\n",
        "    :return: dict of frequent itemsets\n",
        "    \"\"\"\n",
        "    length = len(state['hash_functions'])\n",
        "    buckets = int(psutil.virtual_memory().free * 0.7 / length)\n",
        "    state = State(k=1, lk={}, bitmaps=[], old_buckets=buckets, buckets=buckets, force=False, save=SAVE) + state\n",
        "\n",
        "    while state['k'] == 1 or state['lk']:\n",
        "        lk, bitmaps = get_lk_v3(transactions(), state)\n",
        "        state['lk'] = lk\n",
        "        state['bitmaps'] = bitmaps\n",
        "        state['k'] += 1\n",
        "        old_buckets, buckets = buckets, int(psutil.virtual_memory().free * 0.7 / length)\n",
        "\n",
        "        yield state"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O_Ehb6yDjas"
      },
      "source": [
        "def hash_function1(buckets: int, itemset: Itemset) -> int:\n",
        "    acc = 1\n",
        "    for item in itemset:\n",
        "        acc *= item\n",
        "\n",
        "    return acc % buckets\n",
        "\n",
        "\n",
        "def hash_function2(buckets: int, itemset: Itemset) -> int:\n",
        "    coeff = list(range(1, len(itemset) + 1))\n",
        "\n",
        "    acc = 1\n",
        "    for c, item in zip(coeff, itemset):\n",
        "        acc *= c * item\n",
        "\n",
        "    return acc % buckets"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv3QqPkWKVRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2989a3-5a50-45df-d88b-88a27791439b"
      },
      "source": [
        "algorithm_v3 = pcy_algorithm(lambda: read_csvfile(file), State(threshold=APRIORI_THRESHOLD,\n",
        "                                                               hash_functions=(hash_function1, hash_function2)))\n",
        "\n",
        "singleton_v3 = next(algorithm_v3)['lk']\n",
        "doubleton_v3 = next(algorithm_v3)['lk']\n",
        "triple_v3 = next(algorithm_v3)['lk']\n",
        "quadruple_v3 = next(algorithm_v3)['lk']\n",
        "quintuple_v3 = next(algorithm_v3)['lk']"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:13:12,580]:root: Executing apriori algorithm with 1 items and 30 as threshold\n",
            "[INFO 2021-07-20 11:13:12,582]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n",
            "[INFO 2021-07-20 11:13:33,218]:root: Found 7453 frequent itemsets\n",
            "[INFO 2021-07-20 11:13:33,260]:root: Executing apriori algorithm with 2 items and 30 as threshold\n",
            "[INFO 2021-07-20 11:13:33,263]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v3: 0:00:20.597911\n",
            "Execution time for get_lk_v3: 0:00:20.678678\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:14:03,526]:root: Found 373 frequent itemsets\n",
            "[INFO 2021-07-20 11:14:03,643]:root: Executing apriori algorithm with 3 items and 30 as threshold\n",
            "[INFO 2021-07-20 11:14:03,645]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v3: 0:00:30.248995\n",
            "Execution time for get_lk_v3: 0:00:30.290497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:14:30,824]:root: Found 90 frequent itemsets\n",
            "[INFO 2021-07-20 11:14:30,971]:root: Executing apriori algorithm with 4 items and 30 as threshold\n",
            "[INFO 2021-07-20 11:14:30,973]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v3: 0:00:27.178608\n",
            "Execution time for get_lk_v3: 0:00:27.185568\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:14:52,388]:root: Found 34 frequent itemsets\n",
            "[INFO 2021-07-20 11:14:52,553]:root: Executing apriori algorithm with 5 items and 30 as threshold\n",
            "[INFO 2021-07-20 11:14:52,555]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v3: 0:00:21.414002\n",
            "Execution time for get_lk_v3: 0:00:21.418590\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:15:06,477]:root: Found 5 frequent itemsets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v3: 0:00:13.921718\n",
            "Execution time for get_lk_v3: 0:00:13.925489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmWVL2G0UHiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb5cc15-5fd4-4893-f460-d29cc6dcb7df"
      },
      "source": [
        "if DUMP:\n",
        "    names = read_csv_df(get_path(DATASET_PATH, 'name.basics.tsv.gz'), sep='\\t')\n",
        "    dump_frequent_itemsets_stats(create_temp_df(singleton_v3, names), 1)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(doubleton_v3, names), 2)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(triple_v3, names), 3)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(quadruple_v3, names), 4)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(quintuple_v3, names), 5)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:15:06,605]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/name.basics.tsv.gz with header:True and sep:\t\n",
            "[INFO 2021-07-20 11:15:07,350]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+------------------+\n",
            "|support|   actor1|        actor1Name|\n",
            "+-------+---------+------------------+\n",
            "|    797|nm0103977|      Brahmanandam|\n",
            "|    585|nm0006982|       Adoor Bhasi|\n",
            "|    565|nm0648803|  Matsunosuke Onoe|\n",
            "|    506|nm0305182|      Eddie Garcia|\n",
            "|    436|nm0623427|        Prem Nazir|\n",
            "|    411|nm0793813|      Sung-il Shin|\n",
            "|    391|nm0246703|      Paquito Diaz|\n",
            "|    387|nm0619107|  Masayoshi Nogami|\n",
            "|    380|nm0007123|         Mammootty|\n",
            "|    356|nm7390393|    Aachi Manorama|\n",
            "|    348|nm0046850|           Bahadur|\n",
            "|    344|nm0482320|          Mohanlal|\n",
            "|    330|nm0149822|Mithun Chakraborty|\n",
            "|    323|nm0304262|   Shivaji Ganesan|\n",
            "|    315|nm0706691|       Sultan Rahi|\n",
            "|    313|nm0619309|            Nagesh|\n",
            "|    311|nm0007106|     Shakti Kapoor|\n",
            "|    303|nm0419653|       Jayabharati|\n",
            "|    303|nm0001000|         Tom Byron|\n",
            "|    303|nm0659250|       Pandharibai|\n",
            "+-------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:15:57,646]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+-----------------+---+---+\n",
            "|count|              avg|max|min|\n",
            "+-----+-----------------+---+---+\n",
            "| 7453|57.35167046826781|797| 30|\n",
            "+-----+-----------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:00:50.123674\n",
            "+-------+---------+--------------------+---------+--------------------+\n",
            "|support|   actor1|          actor1Name|   actor2|          actor2Name|\n",
            "+-------+---------+--------------------+---------+--------------------+\n",
            "|    236|nm0006982|         Adoor Bhasi|nm0623427|          Prem Nazir|\n",
            "|    169|nm0006982|         Adoor Bhasi|nm0046850|             Bahadur|\n",
            "|    162|nm0006982|         Adoor Bhasi|nm0419653|         Jayabharati|\n",
            "|    147|nm0648803|    Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|\n",
            "|    126|nm0648803|    Matsunosuke Onoe|nm2373718|    Kitsuraku Arashi|\n",
            "|    122|nm0006982|         Adoor Bhasi|nm0619779|Thikkurisi Sukuma...|\n",
            "|    113|nm2082516|        Kijaku Ôtani|nm2373718|    Kitsuraku Arashi|\n",
            "|    113|nm0648803|    Matsunosuke Onoe|nm2077739|   Suminojo Ichikawa|\n",
            "|    109|nm0046850|             Bahadur|nm0419653|         Jayabharati|\n",
            "|    103|nm0659173|            Panchito|nm1006879|              Dolphy|\n",
            "|    101|nm0619779|Thikkurisi Sukuma...|nm0623427|          Prem Nazir|\n",
            "|    101|nm2077739|   Suminojo Ichikawa|nm2082516|        Kijaku Ôtani|\n",
            "|     97|nm2077739|   Suminojo Ichikawa|nm2373718|    Kitsuraku Arashi|\n",
            "|     97|nm0648803|    Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|\n",
            "|     96|nm0046850|             Bahadur|nm0623427|          Prem Nazir|\n",
            "|     96|nm2366585|       Ritoku Arashi|nm2384746|         Hôshô Bandô|\n",
            "|     92|nm0006982|         Adoor Bhasi|nm0080246|  Paravoor Bharathan|\n",
            "|     92|nm0419653|         Jayabharati|nm0623427|          Prem Nazir|\n",
            "|     90|nm0006982|         Adoor Bhasi|nm1467390|               Meena|\n",
            "|     90|nm1698868|     Enshô Jitsukawa|nm2366585|       Ritoku Arashi|\n",
            "+-------+---------+--------------------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+------------------+---+---+\n",
            "|count|               avg|max|min|\n",
            "+-----+------------------+---+---+\n",
            "|  373|45.501340482573724|236| 30|\n",
            "+-----+------------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:00:54.821413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:16:52,698]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+-------------------+---------+--------------------+---------+------------------+\n",
            "|support|   actor1|         actor1Name|   actor2|          actor2Name|   actor3|        actor3Name|\n",
            "+-------+---------+-------------------+---------+--------------------+---------+------------------+\n",
            "|    112|nm0648803|   Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|    100|nm0648803|   Matsunosuke Onoe|nm2077739|   Suminojo Ichikawa|nm2082516|      Kijaku Ôtani|\n",
            "|     95|nm0648803|   Matsunosuke Onoe|nm2077739|   Suminojo Ichikawa|nm2373718|  Kitsuraku Arashi|\n",
            "|     87|nm2077739|  Suminojo Ichikawa|nm2082516|        Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     80|nm0648803|   Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|nm2082516|      Kijaku Ôtani|\n",
            "|     75|nm0006982|        Adoor Bhasi|nm0046850|             Bahadur|nm0419653|       Jayabharati|\n",
            "|     74|nm0006982|        Adoor Bhasi|nm0619779|Thikkurisi Sukuma...|nm0623427|        Prem Nazir|\n",
            "|     70|nm0648803|   Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|nm2373718|  Kitsuraku Arashi|\n",
            "|     69|nm1698868|    Enshô Jitsukawa|nm2366585|       Ritoku Arashi|nm2384746|       Hôshô Bandô|\n",
            "|     64|nm0648803|   Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|nm2077739| Suminojo Ichikawa|\n",
            "|     63|nm1770187|Sen'nosuke Nakamura|nm2082516|        Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     61|nm0006982|        Adoor Bhasi|nm0419653|         Jayabharati|nm0623427|        Prem Nazir|\n",
            "|     60|nm2366585|      Ritoku Arashi|nm2367854|        Shôzô Arashi|nm2384746|       Hôshô Bandô|\n",
            "|     60|nm0006982|        Adoor Bhasi|nm0046850|             Bahadur|nm0623427|        Prem Nazir|\n",
            "|     58|nm1698868|    Enshô Jitsukawa|nm2366585|       Ritoku Arashi|nm2367854|      Shôzô Arashi|\n",
            "|     58|nm0297793|       Hideo Fujino|nm0945427|     Kaichi Yamamoto|nm2394215|       Takeo Azuma|\n",
            "|     56|nm0648803|   Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|nm2426685|Kakumatsuro Arashi|\n",
            "|     55|nm0648803|   Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|nm2373151|    Chosei Kataoka|\n",
            "|     55|nm1770187|Sen'nosuke Nakamura|nm2077739|   Suminojo Ichikawa|nm2082516|      Kijaku Ôtani|\n",
            "|     54|nm0006982|        Adoor Bhasi|nm0616102|       T.S. Muthaiah|nm0623427|        Prem Nazir|\n",
            "+-------+---------+-------------------+---------+--------------------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-----------------+---+---+\n",
            "|count|              avg|max|min|\n",
            "+-----+-----------------+---+---+\n",
            "|   90|45.44444444444444|112| 30|\n",
            "+-----+-----------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:01:04.672071\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:17:57,697]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+-------------------+---------+-------------------+---------+-----------------+---------+------------------+\n",
            "|support|   actor1|         actor1Name|   actor2|         actor2Name|   actor3|       actor3Name|   actor4|        actor4Name|\n",
            "+-------+---------+-------------------+---------+-------------------+---------+-----------------+---------+------------------+\n",
            "|     86|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     62|nm0648803|   Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     54|nm0648803|   Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2077739|Suminojo Ichikawa|nm2082516|      Kijaku Ôtani|\n",
            "|     51|nm1698868|    Enshô Jitsukawa|nm2366585|      Ritoku Arashi|nm2367854|     Shôzô Arashi|nm2384746|       Hôshô Bandô|\n",
            "|     51|nm0648803|   Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2077739|Suminojo Ichikawa|nm2373718|  Kitsuraku Arashi|\n",
            "|     48|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2373151|   Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     46|nm0648803|   Matsunosuke Onoe|nm1283907|      Utae Nakamura|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     45|nm1770187|Sen'nosuke Nakamura|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     45|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373151|    Chosei Kataoka|\n",
            "|     44|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2373151|   Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     42|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2373718| Kitsuraku Arashi|nm2426685|Kakumatsuro Arashi|\n",
            "|     42|nm2077739|  Suminojo Ichikawa|nm2082516|       Kijaku Ôtani|nm2373151|   Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     38|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2426685|Kakumatsuro Arashi|\n",
            "|     38|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2687024|     Rihaku Arashi|\n",
            "|     37|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2373718| Kitsuraku Arashi|nm2426685|Kakumatsuro Arashi|\n",
            "|     37|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2373718| Kitsuraku Arashi|nm2687024|     Rihaku Arashi|\n",
            "|     36|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2369538|  Ichitarô Kataoka|\n",
            "|     36|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2369538| Ichitarô Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     35|nm0297793|       Hideo Fujino|nm0455938| Teinosuke Kinugasa|nm0945427|  Kaichi Yamamoto|nm2394215|       Takeo Azuma|\n",
            "|     34|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2414317|  Sentarô Nakamura|\n",
            "+-------+---------+-------------------+---------+-------------------+---------+-----------------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+------------------+---+---+\n",
            "|count|               avg|max|min|\n",
            "+-----+------------------+---+---+\n",
            "|   34|39.705882352941174| 86| 30|\n",
            "+-----+------------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:01:12.986955\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:19:11,132]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+----------------+---------+-------------------+---------+-----------------+---------+----------------+---------+------------------+\n",
            "|support|   actor1|      actor1Name|   actor2|         actor2Name|   actor3|       actor3Name|   actor4|      actor4Name|   actor5|        actor5Name|\n",
            "+-------+---------+----------------+---------+-------------------+---------+-----------------+---------+----------------+---------+------------------+\n",
            "|     44|nm0648803|Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2077739|Suminojo Ichikawa|nm2082516|    Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     42|nm0648803|Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373151|  Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     31|nm0648803|Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|Kitsuraku Arashi|nm2426685|Kakumatsuro Arashi|\n",
            "|     30|nm0648803|Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|Kitsuraku Arashi|nm2687024|     Rihaku Arashi|\n",
            "|     30|nm0648803|Matsunosuke Onoe|nm1283907|      Utae Nakamura|nm2077739|Suminojo Ichikawa|nm2082516|    Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "+-------+---------+----------------+---------+-------------------+---------+-----------------+---------+----------------+---------+------------------+\n",
            "\n",
            "+-----+----+---+---+\n",
            "|count| avg|max|min|\n",
            "+-----+----+---+---+\n",
            "|    5|35.4| 44| 30|\n",
            "+-----+----+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:01:23.252726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaOyVEWw-2Zx"
      },
      "source": [
        "## SON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkSY1aEpHzR8"
      },
      "source": [
        "@timer\n",
        "def get_ck_v4(rdd: RDD, algorithm: Callable[[Any], Algorithm],\n",
        "           old_state: Broadcast, new_state: Accumulator) -> Set[Itemset]:\n",
        "    \"\"\"\n",
        "    Scan the chunk and extract candidate frequent itemsets.\n",
        "\n",
        "    :param rdd: rdd which contains transactions\n",
        "    :param algorithm: algorithm to extract frequent itemsets\n",
        "    :param old_state: previous state of the algorithm\n",
        "    :param new_state: next state of the algorithm\n",
        "    :return: candidate frequent itemsets\n",
        "    \"\"\"\n",
        "\n",
        "    def apply_algorithm(index: int, transactions: Iterable[Transaction]) -> CandidateFrequentItemsets:\n",
        "        name = f'partitions_{index}'\n",
        "        if name not in old_state.value:\n",
        "            buckets = [bucket for bucket in transactions]\n",
        "            state = old_state.value\n",
        "            state['threshold'] *= len(buckets) / old_state.value['n']\n",
        "            state['save'] = False\n",
        "            alg = algorithm(lambda: buckets, State(**old_state.value))\n",
        "        else:\n",
        "            state = old_state.value[name]\n",
        "            state['lk'] = old_state.value['lk']\n",
        "            state['save'] = False\n",
        "            alg = algorithm(lambda: transactions, State(**state))\n",
        "\n",
        "        state = next(alg)\n",
        "        new_state.add({name: state.state})\n",
        "        return {itemset: 1 for itemset in state['lk'].items()}\n",
        "\n",
        "    # set is useful to lookup. RDD cannot be used inside mapPartitions\n",
        "    return set(rdd.mapPartitionsWithIndex(apply_algorithm).map(lambda x: x[0]).collect())\n",
        "\n",
        "\n",
        "@timer\n",
        "def get_lk_v4(rdd: RDD, algorithm: Callable[[Any], Algorithm], state: State) -> FrequentItemsets:\n",
        "    \"\"\"\n",
        "    Extract frequent itemsets checking the support.\n",
        "\n",
        "    :param rdd: rdd which contains transactions\n",
        "    :param algorithm: algorithm to extract frequent itemsets\n",
        "    :param state: previous state of the algorithm\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    threshold = state['threshold']\n",
        "    size = state['k']\n",
        "    force = state['force']\n",
        "\n",
        "    path = get_path(RESULTS, f'apriori_{threshold}_{size}', 'csv', 'son', delete=force)\n",
        "\n",
        "    if not is_empty(path):\n",
        "        logging.info('Reading already extracted data')\n",
        "\n",
        "        return read_frequent_itemsets(path)\n",
        "\n",
        "    logging.info(f'Executing apriori algorithm with {size} items and {threshold} as threshold')\n",
        "\n",
        "    old_state = get_spark().sparkContext.broadcast(state.state)\n",
        "    new_state = get_spark().sparkContext.accumulator(state.state, DictParam())\n",
        "    ck = get_spark().sparkContext.broadcast(get_ck_v4(rdd, algorithm, old_state, new_state))\n",
        "    state.update(State(**new_state.value))\n",
        "\n",
        "    def get_support(transactions: Iterable[Transaction]) -> Iterable[Tuple[Itemset, int]]:\n",
        "        accumulator = defaultdict(int)\n",
        "        for row in transactions:\n",
        "            raw_actors = [int(actor[2:]) for actor in row[1].split('|')]\n",
        "            for comb in combinations(sorted(raw_actors), size):\n",
        "                if comb in ck.value:\n",
        "                    accumulator[comb] += 1\n",
        "        return accumulator.items()\n",
        "\n",
        "    lk = dict(rdd.mapPartitions(get_support)\n",
        "              .reduceByKey(lambda x, y: x + y)\n",
        "              .filter(lambda x: x[1] >= threshold)\n",
        "              .collect())\n",
        "    logging.info(f'Found {len(lk)} frequent itemsets')\n",
        "\n",
        "    if state['save']:\n",
        "        save_frequent_itemsets(lk, path)\n",
        "\n",
        "    return lk\n",
        "\n",
        "\n",
        "def son_algorithm(rdd: RDD, algorithm: Callable[[Any], Algorithm], state: State) -> Algorithm:\n",
        "    \"\"\"\n",
        "    Executing son algorithm starting from data and a given threshold.\n",
        "\n",
        "    :param rdd: dataframe which contains transactions\n",
        "    :param algorithm: algorithm to extract frequent itemsets\n",
        "    :param state: state of the algorithm:\n",
        "        - threshold: threshold for the algorithm\n",
        "        - k: size of the itemsets\n",
        "        - lk: frequent itemsets with size k-1\n",
        "        - n: number of distinct buckets\n",
        "        - force: to force recalculating frequent itemsets\n",
        "    :return: dict of frequent itemsets\n",
        "    \"\"\"\n",
        "    state = State(n=rdd.count(), chunks=rdd.getNumPartitions(), k=1, lk={}, force=False, save=SAVE) + state\n",
        "\n",
        "    while state['k'] == 1 or state['lk']:\n",
        "        state['lk'] = get_lk_v4(rdd, algorithm, state)\n",
        "        state['k'] += 1\n",
        "\n",
        "        yield state"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDrEyWbnH0KW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03910897-7efb-41fc-8358-a3f24251f455"
      },
      "source": [
        "data = read_csv_rdd(file).repartition(SON_CHUNKS).map(lambda row: (row[0], row[1])).persist()\n",
        "\n",
        "algorithm_v4 = son_algorithm(data, apriori_algorithm_v2, State(threshold=APRIORI_THRESHOLD))\n",
        "\n",
        "singleton_v4 = next(algorithm_v4)['lk']\n",
        "doubleton_v4 = next(algorithm_v4)['lk']\n",
        "triple_v4 = next(algorithm_v4)['lk']\n",
        "quadruple_v4 = next(algorithm_v4)['lk']\n",
        "quintuple_v4 = next(algorithm_v4)['lk']"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:20:34,509]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv with sep:,\n",
            "[INFO 2021-07-20 11:20:38,270]:root: Executing apriori algorithm with 1 items and 30 as threshold\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v4: 0:00:03.265958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:20:49,163]:root: Found 7453 frequent itemsets\n",
            "[INFO 2021-07-20 11:20:49,168]:root: Executing apriori algorithm with 2 items and 30 as threshold\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk_v4: 0:00:10.896021\n",
            "Execution time for get_ck_v4: 0:00:05.264171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:20:59,536]:root: Found 373 frequent itemsets\n",
            "[INFO 2021-07-20 11:20:59,539]:root: Executing apriori algorithm with 3 items and 30 as threshold\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk_v4: 0:00:10.369836\n",
            "Execution time for get_ck_v4: 0:00:05.339171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:21:07,828]:root: Found 90 frequent itemsets\n",
            "[INFO 2021-07-20 11:21:07,831]:root: Executing apriori algorithm with 4 items and 30 as threshold\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk_v4: 0:00:08.292175\n",
            "Execution time for get_ck_v4: 0:00:05.410620\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:21:16,093]:root: Found 34 frequent itemsets\n",
            "[INFO 2021-07-20 11:21:16,096]:root: Executing apriori algorithm with 5 items and 30 as threshold\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk_v4: 0:00:08.264625\n",
            "Execution time for get_ck_v4: 0:00:04.952236\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:21:23,379]:root: Found 5 frequent itemsets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk_v4: 0:00:07.284737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggXUaIg4H7ns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f539e480-5cfe-4db9-9508-900c966c3682"
      },
      "source": [
        "if DUMP:\n",
        "    names = read_csv_df(get_path(DATASET_PATH, 'name.basics.tsv.gz'), sep='\\t')\n",
        "    dump_frequent_itemsets_stats(create_temp_df(singleton_v4, names), 1)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(doubleton_v4, names), 2)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(triple_v4, names), 3)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(quadruple_v4, names), 4)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(quintuple_v4, names), 5)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:21:23,397]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/name.basics.tsv.gz with header:True and sep:\t\n",
            "[INFO 2021-07-20 11:21:23,884]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+------------------+\n",
            "|support|   actor1|        actor1Name|\n",
            "+-------+---------+------------------+\n",
            "|    797|nm0103977|      Brahmanandam|\n",
            "|    585|nm0006982|       Adoor Bhasi|\n",
            "|    565|nm0648803|  Matsunosuke Onoe|\n",
            "|    506|nm0305182|      Eddie Garcia|\n",
            "|    436|nm0623427|        Prem Nazir|\n",
            "|    411|nm0793813|      Sung-il Shin|\n",
            "|    391|nm0246703|      Paquito Diaz|\n",
            "|    387|nm0619107|  Masayoshi Nogami|\n",
            "|    380|nm0007123|         Mammootty|\n",
            "|    356|nm7390393|    Aachi Manorama|\n",
            "|    348|nm0046850|           Bahadur|\n",
            "|    344|nm0482320|          Mohanlal|\n",
            "|    330|nm0149822|Mithun Chakraborty|\n",
            "|    323|nm0304262|   Shivaji Ganesan|\n",
            "|    315|nm0706691|       Sultan Rahi|\n",
            "|    313|nm0619309|            Nagesh|\n",
            "|    311|nm0007106|     Shakti Kapoor|\n",
            "|    303|nm0001000|         Tom Byron|\n",
            "|    303|nm0419653|       Jayabharati|\n",
            "|    303|nm0659250|       Pandharibai|\n",
            "+-------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:22:15,555]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+-----------------+---+---+\n",
            "|count|              avg|max|min|\n",
            "+-----+-----------------+---+---+\n",
            "| 7453|57.35167046826781|797| 30|\n",
            "+-----+-----------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:00:51.514500\n",
            "+-------+---------+--------------------+---------+--------------------+\n",
            "|support|   actor1|          actor1Name|   actor2|          actor2Name|\n",
            "+-------+---------+--------------------+---------+--------------------+\n",
            "|    236|nm0006982|         Adoor Bhasi|nm0623427|          Prem Nazir|\n",
            "|    169|nm0006982|         Adoor Bhasi|nm0046850|             Bahadur|\n",
            "|    162|nm0006982|         Adoor Bhasi|nm0419653|         Jayabharati|\n",
            "|    147|nm0648803|    Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|\n",
            "|    126|nm0648803|    Matsunosuke Onoe|nm2373718|    Kitsuraku Arashi|\n",
            "|    122|nm0006982|         Adoor Bhasi|nm0619779|Thikkurisi Sukuma...|\n",
            "|    113|nm2082516|        Kijaku Ôtani|nm2373718|    Kitsuraku Arashi|\n",
            "|    113|nm0648803|    Matsunosuke Onoe|nm2077739|   Suminojo Ichikawa|\n",
            "|    109|nm0046850|             Bahadur|nm0419653|         Jayabharati|\n",
            "|    103|nm0659173|            Panchito|nm1006879|              Dolphy|\n",
            "|    101|nm0619779|Thikkurisi Sukuma...|nm0623427|          Prem Nazir|\n",
            "|    101|nm2077739|   Suminojo Ichikawa|nm2082516|        Kijaku Ôtani|\n",
            "|     97|nm0648803|    Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|\n",
            "|     97|nm2077739|   Suminojo Ichikawa|nm2373718|    Kitsuraku Arashi|\n",
            "|     96|nm0046850|             Bahadur|nm0623427|          Prem Nazir|\n",
            "|     96|nm2366585|       Ritoku Arashi|nm2384746|         Hôshô Bandô|\n",
            "|     92|nm0419653|         Jayabharati|nm0623427|          Prem Nazir|\n",
            "|     92|nm0006982|         Adoor Bhasi|nm0080246|  Paravoor Bharathan|\n",
            "|     90|nm1698868|     Enshô Jitsukawa|nm2366585|       Ritoku Arashi|\n",
            "|     90|nm0006982|         Adoor Bhasi|nm1467390|               Meena|\n",
            "+-------+---------+--------------------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+------------------+---+---+\n",
            "|count|               avg|max|min|\n",
            "+-----+------------------+---+---+\n",
            "|  373|45.501340482573724|236| 30|\n",
            "+-----+------------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:01:00.313108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:23:16,209]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+-------------------+---------+--------------------+---------+------------------+\n",
            "|support|   actor1|         actor1Name|   actor2|          actor2Name|   actor3|        actor3Name|\n",
            "+-------+---------+-------------------+---------+--------------------+---------+------------------+\n",
            "|    112|nm0648803|   Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|    100|nm0648803|   Matsunosuke Onoe|nm2077739|   Suminojo Ichikawa|nm2082516|      Kijaku Ôtani|\n",
            "|     95|nm0648803|   Matsunosuke Onoe|nm2077739|   Suminojo Ichikawa|nm2373718|  Kitsuraku Arashi|\n",
            "|     87|nm2077739|  Suminojo Ichikawa|nm2082516|        Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     80|nm0648803|   Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|nm2082516|      Kijaku Ôtani|\n",
            "|     75|nm0006982|        Adoor Bhasi|nm0046850|             Bahadur|nm0419653|       Jayabharati|\n",
            "|     74|nm0006982|        Adoor Bhasi|nm0619779|Thikkurisi Sukuma...|nm0623427|        Prem Nazir|\n",
            "|     70|nm0648803|   Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|nm2373718|  Kitsuraku Arashi|\n",
            "|     69|nm1698868|    Enshô Jitsukawa|nm2366585|       Ritoku Arashi|nm2384746|       Hôshô Bandô|\n",
            "|     64|nm0648803|   Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|nm2077739| Suminojo Ichikawa|\n",
            "|     63|nm1770187|Sen'nosuke Nakamura|nm2082516|        Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     61|nm0006982|        Adoor Bhasi|nm0419653|         Jayabharati|nm0623427|        Prem Nazir|\n",
            "|     60|nm2366585|      Ritoku Arashi|nm2367854|        Shôzô Arashi|nm2384746|       Hôshô Bandô|\n",
            "|     60|nm0006982|        Adoor Bhasi|nm0046850|             Bahadur|nm0623427|        Prem Nazir|\n",
            "|     58|nm1698868|    Enshô Jitsukawa|nm2366585|       Ritoku Arashi|nm2367854|      Shôzô Arashi|\n",
            "|     58|nm0297793|       Hideo Fujino|nm0945427|     Kaichi Yamamoto|nm2394215|       Takeo Azuma|\n",
            "|     56|nm0648803|   Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|nm2426685|Kakumatsuro Arashi|\n",
            "|     55|nm0648803|   Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|nm2373151|    Chosei Kataoka|\n",
            "|     55|nm1770187|Sen'nosuke Nakamura|nm2077739|   Suminojo Ichikawa|nm2082516|      Kijaku Ôtani|\n",
            "|     54|nm0006982|        Adoor Bhasi|nm0616102|       T.S. Muthaiah|nm0623427|        Prem Nazir|\n",
            "+-------+---------+-------------------+---------+--------------------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-----------------+---+---+\n",
            "|count|              avg|max|min|\n",
            "+-----+-----------------+---+---+\n",
            "|   90|45.44444444444444|112| 30|\n",
            "+-----+-----------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:01:07.399133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:24:23,884]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+-------------------+---------+-------------------+---------+-----------------+---------+------------------+\n",
            "|support|   actor1|         actor1Name|   actor2|         actor2Name|   actor3|       actor3Name|   actor4|        actor4Name|\n",
            "+-------+---------+-------------------+---------+-------------------+---------+-----------------+---------+------------------+\n",
            "|     86|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     62|nm0648803|   Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     54|nm0648803|   Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2077739|Suminojo Ichikawa|nm2082516|      Kijaku Ôtani|\n",
            "|     51|nm1698868|    Enshô Jitsukawa|nm2366585|      Ritoku Arashi|nm2367854|     Shôzô Arashi|nm2384746|       Hôshô Bandô|\n",
            "|     51|nm0648803|   Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2077739|Suminojo Ichikawa|nm2373718|  Kitsuraku Arashi|\n",
            "|     48|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2373151|   Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     46|nm0648803|   Matsunosuke Onoe|nm1283907|      Utae Nakamura|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     45|nm1770187|Sen'nosuke Nakamura|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     45|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373151|    Chosei Kataoka|\n",
            "|     44|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2373151|   Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     42|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2373718| Kitsuraku Arashi|nm2426685|Kakumatsuro Arashi|\n",
            "|     42|nm2077739|  Suminojo Ichikawa|nm2082516|       Kijaku Ôtani|nm2373151|   Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     38|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2426685|Kakumatsuro Arashi|\n",
            "|     38|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2687024|     Rihaku Arashi|\n",
            "|     37|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2373718| Kitsuraku Arashi|nm2426685|Kakumatsuro Arashi|\n",
            "|     37|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2373718| Kitsuraku Arashi|nm2687024|     Rihaku Arashi|\n",
            "|     36|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2369538|  Ichitarô Kataoka|\n",
            "|     36|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2369538| Ichitarô Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     35|nm0297793|       Hideo Fujino|nm0455938| Teinosuke Kinugasa|nm0945427|  Kaichi Yamamoto|nm2394215|       Takeo Azuma|\n",
            "|     34|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2414317|  Sentarô Nakamura|\n",
            "+-------+---------+-------------------+---------+-------------------+---------+-----------------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+------------------+---+---+\n",
            "|count|               avg|max|min|\n",
            "+-----+------------------+---+---+\n",
            "|   34|39.705882352941174| 86| 30|\n",
            "+-----+------------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:01:14.714014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:25:39,035]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+----------------+---------+-------------------+---------+-----------------+---------+----------------+---------+------------------+\n",
            "|support|   actor1|      actor1Name|   actor2|         actor2Name|   actor3|       actor3Name|   actor4|      actor4Name|   actor5|        actor5Name|\n",
            "+-------+---------+----------------+---------+-------------------+---------+-----------------+---------+----------------+---------+------------------+\n",
            "|     44|nm0648803|Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2077739|Suminojo Ichikawa|nm2082516|    Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     42|nm0648803|Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373151|  Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     31|nm0648803|Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|Kitsuraku Arashi|nm2426685|Kakumatsuro Arashi|\n",
            "|     30|nm0648803|Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|Kitsuraku Arashi|nm2687024|     Rihaku Arashi|\n",
            "|     30|nm0648803|Matsunosuke Onoe|nm1283907|      Utae Nakamura|nm2077739|Suminojo Ichikawa|nm2082516|    Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "+-------+---------+----------------+---------+-------------------+---------+-----------------+---------+----------------+---------+------------------+\n",
            "\n",
            "+-----+----+---+---+\n",
            "|count| avg|max|min|\n",
            "+-----+----+---+---+\n",
            "|    5|35.4| 44| 30|\n",
            "+-----+----+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:01:25.051961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR-eXIyFgxpM"
      },
      "source": [
        "## Toivonen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZSvkARrtFME"
      },
      "source": [
        "@timer\n",
        "def get_ck_v5(transactions: Iterator[Transaction], k: int,\n",
        "           monotonicity_filter: Callable[[Itemset], int]) -> CandidateFrequentItemsets:\n",
        "    \"\"\"\n",
        "    Scan the sample and extract candidate frequent itemsets checking the monotonicity condition.\n",
        "\n",
        "    :param transactions: iterator of transactions\n",
        "    :param k: size of the itemset\n",
        "    :param monotonicity_filter: filter for the monotonicity condition\n",
        "    :return: candidate frequent itemsets\n",
        "    \"\"\"\n",
        "    accumulator = defaultdict(int)\n",
        "    for row in transactions:\n",
        "        raw_actors = [int(actor[2:]) for actor in row[1].split('|')]\n",
        "        for comb in combinations(sorted(raw_actors), k):\n",
        "            if monotonicity_filter(comb):\n",
        "                accumulator[comb] += 1\n",
        "    return accumulator\n",
        "\n",
        "\n",
        "@timer\n",
        "def get_lk_v5(transactions: Iterator[Transaction], sample: List[Transaction], state: State) -> Optional[FrequentItemsets]:\n",
        "    \"\"\"\n",
        "    Extract frequent itemsets checking the support.\n",
        "\n",
        "    :param transactions: iterator of transactions\n",
        "    :param sample: sample of all transactions\n",
        "    :param state: state of the algorithm\n",
        "    :return: frequent itemsets\n",
        "    \"\"\"\n",
        "    n = state['n']\n",
        "    size = state['k']\n",
        "    old_lk = state['lk']\n",
        "    threshold_adjust = state['threshold_adjust']\n",
        "    threshold = threshold_adjust * len(sample) / n * state['threshold']\n",
        "    force = state['force']\n",
        "\n",
        "    path = get_path(RESULTS, f'apriori_{threshold}_{size}', 'csv', 'toivonen', delete=force)\n",
        "\n",
        "    if not is_empty(path):\n",
        "        logging.info('Reading already extracted data')\n",
        "\n",
        "        return read_frequent_itemsets(path)\n",
        "\n",
        "    logging.info(f'Executing apriori algorithm with {size} items and {threshold} as threshold')\n",
        "\n",
        "    if size == 1:\n",
        "        ck = get_ck_v5(sample, size, lambda _: True)\n",
        "    else:\n",
        "        ck = get_ck_v5(sample, size,\n",
        "                    lambda itemset: all([item in old_lk for item in combinations(itemset, size - 1)]))\n",
        "\n",
        "    lk = {item\n",
        "          for item, support in ck.items()\n",
        "          if support >= threshold}\n",
        "    negative_border = {item\n",
        "            for item, support in ck.items()\n",
        "            if support < threshold}\n",
        "\n",
        "    full_support = last_full_scan(transactions, size, lk, negative_border)\n",
        "    real_lk = {item: support\n",
        "          for item, support in full_support.items()\n",
        "          if support >= state['threshold']}\n",
        "\n",
        "    common = set(real_lk.keys()) & negative_border\n",
        "    if common:\n",
        "        logging.warning(f'Found {len(common)} frequent itemsets in the negative border.\\n'\n",
        "                        f'Example: {[(i, ck[i], full_support[i]) for i in list(common)[:10]]}')\n",
        "        return None\n",
        "\n",
        "    not_found = set(real_lk.keys()) - lk\n",
        "    if not_found:\n",
        "        logging.warning(f'Found {len(not_found)} frequent itemsets not found.\\n'\n",
        "                        f'Example: {not_found}')\n",
        "        return None\n",
        "\n",
        "    logging.info(f'Found {len(real_lk)} frequent itemsets')\n",
        "\n",
        "    if state['save']:\n",
        "        save_frequent_itemsets(real_lk, path)\n",
        "\n",
        "    return real_lk\n",
        "\n",
        "\n",
        "def sampling(df: DataFrame, k: int) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Get a random sample of the dataframe of size k.\n",
        "\n",
        "    :param df: dataframe to be sampled\n",
        "    :param k: size of the sample\n",
        "    :return: sampled dataframe\n",
        "    \"\"\"\n",
        "    n = df.count()\n",
        "    df = (df\n",
        "          .withColumn('fake', F.lit(0))\n",
        "          .withColumn('row', F.row_number().over(Window.partitionBy('fake').orderBy('fake')))\n",
        "          .drop('fake')\n",
        "          )\n",
        "    sample = get_spark().createDataFrame(random.choices(range(1, n + 1), k=k), T.IntegerType())\n",
        "    return df.join(sample, on=df.row == sample.value).drop('row', 'value')\n",
        "\n",
        "\n",
        "def last_full_scan(transactions: Iterator[Transaction], k: int,\n",
        "                   lk: Set[Itemset], negative_border: Set[Itemset]) -> FrequentItemsets:\n",
        "    \"\"\"\n",
        "    Scan the file and extract candidate frequent itemsets.\n",
        "\n",
        "    :param transactions: iterator of transactions\n",
        "    :param k: size of the itemset\n",
        "    :param lk: candidate frequent itemset\n",
        "    :param negative_border: infrequent itemset\n",
        "    :return: candidate frequent itemsets\n",
        "    \"\"\"\n",
        "    accumulator = defaultdict(int)\n",
        "    for row in transactions:\n",
        "        raw_actors = [int(actor[2:]) for actor in row[1].split('|')]\n",
        "        for comb in combinations(sorted(raw_actors), k):\n",
        "            if comb in lk or comb in negative_border:\n",
        "                accumulator[comb] += 1\n",
        "    return accumulator\n",
        "\n",
        "\n",
        "def toivonen_algorithm(transactions: Callable[[], Iterator[Transaction]], \n",
        "                       sample: Iterator[Transaction],  state: State) -> Algorithm:\n",
        "    \"\"\"\n",
        "    Executing apriori algorithm starting from data and a given threshold.\n",
        "\n",
        "    :param transactions: callable to an iterator of transactions\n",
        "    :param state: state of the algorithm:\n",
        "        - threshold: threshold for the apriori algorithm\n",
        "        - k: size of the itemsets\n",
        "        - lk: frequent itemsets with size k-1\n",
        "        - force: to force recalculating frequent itemsets\n",
        "    :return: dict of frequent itemsets\n",
        "    \"\"\"\n",
        "    state = State(k=1, lk={}, force=False, save=SAVE) + state\n",
        "\n",
        "    while state['k'] == 1 or state['lk']:\n",
        "        state['lk'] = get_lk_v5(transactions(), sample, state)\n",
        "        if state['lk'] is None:\n",
        "            return state\n",
        "        state['k'] += 1\n",
        "\n",
        "        yield state"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dObONzfenEZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5813e28-6df6-4497-8bb2-a4ba09f36b54"
      },
      "source": [
        "df1 = read_csv_df(file, header=False)\n",
        "n = df1.count()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:27:04,378]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv with header:False and sep:,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkwn4u9geD2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfbdd9fc-dc40-43ab-ce96-41adcb4fd78a"
      },
      "source": [
        "for i in range(TOIVONEN_MAX_ITERATIONS):\n",
        "    print(f'\\tIteration number {i}')\n",
        "    sample = [(row['_c0'], row['_c1']) for row in sampling(df1, TOIVONEN_SIZE_SAMPLE).collect()]\n",
        "    algorithm_v5 = toivonen_algorithm(lambda: read_csvfile(file), sample,\n",
        "                                    State(threshold_adjust=TOIVONEN_THRESHOLD_ADJUST,\n",
        "                                          threshold=APRIORI_THRESHOLD, n=n))\n",
        "    try:\n",
        "        singleton_v5 = next(algorithm_v5)['lk']\n",
        "        doubleton_v5 = next(algorithm_v5)['lk']\n",
        "        triple_v5 = next(algorithm_v5)['lk']\n",
        "        quadruple_v5 = next(algorithm_v5)['lk']\n",
        "        quintuple_v5 = next(algorithm_v5)['lk']\n",
        "        print('Toivonen completed')\n",
        "        break\n",
        "    except StopIteration:\n",
        "      pass"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tIteration number 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:32:24,164]:root: Executing apriori algorithm with 1 items and 3.047549389347291 as threshold\n",
            "[INFO 2021-07-20 11:32:24,486]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v5: 0:00:00.279458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING 2021-07-20 11:32:27,436]:root: Found 1394 frequent itemsets in the negative border.\n",
            "Example: [((687958,), 3, 38), ((1069512,), 3, 32), ((131209,), 2, 45), ((223591,), 3, 30), ((284608,), 3, 34), ((719749,), 3, 51), ((888978,), 3, 36), ((661293,), 2, 32), ((7545,), 3, 34), ((176039,), 2, 31)]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk_v5: 0:00:03.290164\n",
            "\tIteration number 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:32:32,620]:root: Executing apriori algorithm with 1 items and 3.047549389347291 as threshold\n",
            "[INFO 2021-07-20 11:32:32,953]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v5: 0:00:00.287669\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING 2021-07-20 11:32:35,956]:root: Found 1361 frequent itemsets in the negative border.\n",
            "Example: [((687958,), 1, 38), ((1186,), 3, 43), ((1069512,), 1, 32), ((131209,), 3, 45), ((137638,), 3, 39), ((7217,), 2, 42), ((661293,), 2, 32), ((720979,), 2, 34), ((1774,), 3, 38), ((7545,), 3, 34)]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk_v5: 0:00:03.355296\n",
            "\tIteration number 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:32:40,335]:root: Executing apriori algorithm with 1 items and 3.047549389347291 as threshold\n",
            "[INFO 2021-07-20 11:32:40,652]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v5: 0:00:00.272786\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING 2021-07-20 11:32:43,616]:root: Found 1453 frequent itemsets in the negative border.\n",
            "Example: [((3132784,), 2, 51), ((36208,), 3, 34), ((284608,), 1, 34), ((820865,), 3, 46), ((1259779,), 2, 36), ((661293,), 3, 32), ((1774,), 3, 38), ((387340,), 3, 31), ((176039,), 3, 31), ((748430,), 3, 31)]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk_v5: 0:00:03.296022\n",
            "\tIteration number 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:32:48,243]:root: Executing apriori algorithm with 1 items and 3.047549389347291 as threshold\n",
            "[INFO 2021-07-20 11:32:48,558]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v5: 0:00:00.268261\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING 2021-07-20 11:32:51,514]:root: Found 1412 frequent itemsets in the negative border.\n",
            "Example: [((1227581,), 1, 37), ((135624,), 3, 35), ((1069512,), 3, 32), ((36208,), 3, 34), ((223591,), 2, 30), ((705384,), 1, 39), ((661293,), 2, 32), ((720979,), 1, 34), ((1774,), 3, 38), ((248890,), 3, 44)]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk_v5: 0:00:03.289923\n",
            "\tIteration number 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:32:56,289]:root: Executing apriori algorithm with 1 items and 3.047549389347291 as threshold\n",
            "[INFO 2021-07-20 11:32:56,600]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v5: 0:00:00.271795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING 2021-07-20 11:32:59,526]:root: Found 1456 frequent itemsets in the negative border.\n",
            "Example: [((279072,), 3, 55), ((284608,), 3, 34), ((595115,), 3, 58), ((888978,), 3, 36), ((1774,), 3, 38), ((725,), 2, 30), ((631102,), 1, 43), ((1993,), 3, 47), ((1962458,), 2, 31), ((837430,), 3, 40)]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk_v5: 0:00:03.253238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjaLhjR-ERcK",
        "outputId": "960dac6e-b79f-4f4a-989c-db17c5d2dfae"
      },
      "source": [
        "for i in range(TOIVONEN_MAX_ITERATIONS):\n",
        "    print(f'\\tIteration number {i}')\n",
        "    sample = [(row['_c0'], row['_c1']) for row in sampling(df1, 200000).collect()]\n",
        "    algorithm_v5 = toivonen_algorithm(lambda: read_csvfile(file), sample,\n",
        "                                    State(threshold_adjust=0.25,\n",
        "                                          threshold=APRIORI_THRESHOLD, n=n))\n",
        "    try:\n",
        "        singleton_v5 = next(algorithm_v5)['lk']\n",
        "        doubleton_v5 = next(algorithm_v5)['lk']\n",
        "        triple_v5 = next(algorithm_v5)['lk']\n",
        "        quadruple_v5 = next(algorithm_v5)['lk']\n",
        "        quintuple_v5 = next(algorithm_v5)['lk']\n",
        "        print('Toivonen completed')\n",
        "        break\n",
        "    except StopIteration:\n",
        "      pass"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tIteration number 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:33:06,862]:root: Executing apriori algorithm with 1 items and 3.8094367366841144 as threshold\n",
            "[INFO 2021-07-20 11:33:08,088]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v5: 0:00:01.113258\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:33:11,621]:root: Found 7453 frequent itemsets\n",
            "[INFO 2021-07-20 11:33:11,658]:root: Executing apriori algorithm with 2 items and 3.8094367366841144 as threshold\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk_v5: 0:00:04.793771\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:33:13,987]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v5: 0:00:02.285314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:33:16,954]:root: Found 373 frequent itemsets\n",
            "[INFO 2021-07-20 11:33:16,976]:root: Executing apriori algorithm with 3 items and 3.8094367366841144 as threshold\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk_v5: 0:00:05.317190\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:33:19,517]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v5: 0:00:02.539058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:33:21,646]:root: Found 90 frequent itemsets\n",
            "[INFO 2021-07-20 11:33:21,649]:root: Executing apriori algorithm with 4 items and 3.8094367366841144 as threshold\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk_v5: 0:00:04.672669\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:33:24,284]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v5: 0:00:02.631854\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:33:26,266]:root: Found 34 frequent itemsets\n",
            "[INFO 2021-07-20 11:33:26,268]:root: Executing apriori algorithm with 5 items and 3.8094367366841144 as threshold\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk_v5: 0:00:04.618827\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:33:28,682]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/raw/csv/part-00000-52e9e128-7801-425a-aba9-349daecf1e8d-c000.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_ck_v5: 0:00:02.410882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:33:30,814]:root: Found 5 frequent itemsets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time for get_lk_v5: 0:00:04.548032\n",
            "Toivonen completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Veb2Ksh7gNqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60549aff-c7f7-4aee-9419-1f7529a27ffc"
      },
      "source": [
        "if DUMP:\n",
        "    names = read_csv_df(get_path(DATASET_PATH, 'name.basics.tsv.gz'), sep='\\t')\n",
        "    dump_frequent_itemsets_stats(create_temp_df(singleton_v5, names), 1)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(doubleton_v5, names), 2)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(triple_v5, names), 3)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(quadruple_v5, names), 4)\n",
        "    dump_frequent_itemsets_stats(create_temp_df(quintuple_v5, names), 5)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:33:30,833]:root: Reading csv path /content/datasets/ashirwadsangwan/imdb-dataset/name.basics.tsv.gz with header:True and sep:\t\n",
            "[INFO 2021-07-20 11:33:31,533]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+------------------+\n",
            "|support|   actor1|        actor1Name|\n",
            "+-------+---------+------------------+\n",
            "|    797|nm0103977|      Brahmanandam|\n",
            "|    585|nm0006982|       Adoor Bhasi|\n",
            "|    565|nm0648803|  Matsunosuke Onoe|\n",
            "|    506|nm0305182|      Eddie Garcia|\n",
            "|    436|nm0623427|        Prem Nazir|\n",
            "|    411|nm0793813|      Sung-il Shin|\n",
            "|    391|nm0246703|      Paquito Diaz|\n",
            "|    387|nm0619107|  Masayoshi Nogami|\n",
            "|    380|nm0007123|         Mammootty|\n",
            "|    356|nm7390393|    Aachi Manorama|\n",
            "|    348|nm0046850|           Bahadur|\n",
            "|    344|nm0482320|          Mohanlal|\n",
            "|    330|nm0149822|Mithun Chakraborty|\n",
            "|    323|nm0304262|   Shivaji Ganesan|\n",
            "|    315|nm0706691|       Sultan Rahi|\n",
            "|    313|nm0619309|            Nagesh|\n",
            "|    311|nm0007106|     Shakti Kapoor|\n",
            "|    303|nm0001000|         Tom Byron|\n",
            "|    303|nm0419653|       Jayabharati|\n",
            "|    303|nm0659250|       Pandharibai|\n",
            "+-------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:34:26,825]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+-----------------+---+---+\n",
            "|count|              avg|max|min|\n",
            "+-----+-----------------+---+---+\n",
            "| 7453|57.35167046826781|797| 30|\n",
            "+-----+-----------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:00:55.114554\n",
            "+-------+---------+--------------------+---------+--------------------+\n",
            "|support|   actor1|          actor1Name|   actor2|          actor2Name|\n",
            "+-------+---------+--------------------+---------+--------------------+\n",
            "|    236|nm0006982|         Adoor Bhasi|nm0623427|          Prem Nazir|\n",
            "|    169|nm0006982|         Adoor Bhasi|nm0046850|             Bahadur|\n",
            "|    162|nm0006982|         Adoor Bhasi|nm0419653|         Jayabharati|\n",
            "|    147|nm0648803|    Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|\n",
            "|    126|nm0648803|    Matsunosuke Onoe|nm2373718|    Kitsuraku Arashi|\n",
            "|    122|nm0006982|         Adoor Bhasi|nm0619779|Thikkurisi Sukuma...|\n",
            "|    113|nm2082516|        Kijaku Ôtani|nm2373718|    Kitsuraku Arashi|\n",
            "|    113|nm0648803|    Matsunosuke Onoe|nm2077739|   Suminojo Ichikawa|\n",
            "|    109|nm0046850|             Bahadur|nm0419653|         Jayabharati|\n",
            "|    103|nm0659173|            Panchito|nm1006879|              Dolphy|\n",
            "|    101|nm2077739|   Suminojo Ichikawa|nm2082516|        Kijaku Ôtani|\n",
            "|    101|nm0619779|Thikkurisi Sukuma...|nm0623427|          Prem Nazir|\n",
            "|     97|nm0648803|    Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|\n",
            "|     97|nm2077739|   Suminojo Ichikawa|nm2373718|    Kitsuraku Arashi|\n",
            "|     96|nm2366585|       Ritoku Arashi|nm2384746|         Hôshô Bandô|\n",
            "|     96|nm0046850|             Bahadur|nm0623427|          Prem Nazir|\n",
            "|     92|nm0006982|         Adoor Bhasi|nm0080246|  Paravoor Bharathan|\n",
            "|     92|nm0419653|         Jayabharati|nm0623427|          Prem Nazir|\n",
            "|     90|nm1698868|     Enshô Jitsukawa|nm2366585|       Ritoku Arashi|\n",
            "|     90|nm0006982|         Adoor Bhasi|nm1467390|               Meena|\n",
            "+-------+---------+--------------------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+------------------+---+---+\n",
            "|count|               avg|max|min|\n",
            "+-----+------------------+---+---+\n",
            "|  373|45.501340482573724|236| 30|\n",
            "+-----+------------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:01:05.080150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:35:32,117]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+-------------------+---------+--------------------+---------+------------------+\n",
            "|support|   actor1|         actor1Name|   actor2|          actor2Name|   actor3|        actor3Name|\n",
            "+-------+---------+-------------------+---------+--------------------+---------+------------------+\n",
            "|    112|nm0648803|   Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|    100|nm0648803|   Matsunosuke Onoe|nm2077739|   Suminojo Ichikawa|nm2082516|      Kijaku Ôtani|\n",
            "|     95|nm0648803|   Matsunosuke Onoe|nm2077739|   Suminojo Ichikawa|nm2373718|  Kitsuraku Arashi|\n",
            "|     87|nm2077739|  Suminojo Ichikawa|nm2082516|        Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     80|nm0648803|   Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|nm2082516|      Kijaku Ôtani|\n",
            "|     75|nm0006982|        Adoor Bhasi|nm0046850|             Bahadur|nm0419653|       Jayabharati|\n",
            "|     74|nm0006982|        Adoor Bhasi|nm0619779|Thikkurisi Sukuma...|nm0623427|        Prem Nazir|\n",
            "|     70|nm0648803|   Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|nm2373718|  Kitsuraku Arashi|\n",
            "|     69|nm1698868|    Enshô Jitsukawa|nm2366585|       Ritoku Arashi|nm2384746|       Hôshô Bandô|\n",
            "|     64|nm0648803|   Matsunosuke Onoe|nm1770187| Sen'nosuke Nakamura|nm2077739| Suminojo Ichikawa|\n",
            "|     63|nm1770187|Sen'nosuke Nakamura|nm2082516|        Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     61|nm0006982|        Adoor Bhasi|nm0419653|         Jayabharati|nm0623427|        Prem Nazir|\n",
            "|     60|nm2366585|      Ritoku Arashi|nm2367854|        Shôzô Arashi|nm2384746|       Hôshô Bandô|\n",
            "|     60|nm0006982|        Adoor Bhasi|nm0046850|             Bahadur|nm0623427|        Prem Nazir|\n",
            "|     58|nm1698868|    Enshô Jitsukawa|nm2366585|       Ritoku Arashi|nm2367854|      Shôzô Arashi|\n",
            "|     58|nm0297793|       Hideo Fujino|nm0945427|     Kaichi Yamamoto|nm2394215|       Takeo Azuma|\n",
            "|     56|nm0648803|   Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|nm2426685|Kakumatsuro Arashi|\n",
            "|     55|nm0648803|   Matsunosuke Onoe|nm2082516|        Kijaku Ôtani|nm2373151|    Chosei Kataoka|\n",
            "|     55|nm1770187|Sen'nosuke Nakamura|nm2077739|   Suminojo Ichikawa|nm2082516|      Kijaku Ôtani|\n",
            "|     54|nm0006982|        Adoor Bhasi|nm0616102|       T.S. Muthaiah|nm0623427|        Prem Nazir|\n",
            "+-------+---------+-------------------+---------+--------------------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-----------------+---+---+\n",
            "|count|              avg|max|min|\n",
            "+-----+-----------------+---+---+\n",
            "|   90|45.44444444444444|112| 30|\n",
            "+-----+-----------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:01:12.351691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:36:44,867]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+-------------------+---------+-------------------+---------+-----------------+---------+------------------+\n",
            "|support|   actor1|         actor1Name|   actor2|         actor2Name|   actor3|       actor3Name|   actor4|        actor4Name|\n",
            "+-------+---------+-------------------+---------+-------------------+---------+-----------------+---------+------------------+\n",
            "|     86|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     62|nm0648803|   Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     54|nm0648803|   Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2077739|Suminojo Ichikawa|nm2082516|      Kijaku Ôtani|\n",
            "|     51|nm1698868|    Enshô Jitsukawa|nm2366585|      Ritoku Arashi|nm2367854|     Shôzô Arashi|nm2384746|       Hôshô Bandô|\n",
            "|     51|nm0648803|   Matsunosuke Onoe|nm1770187|Sen'nosuke Nakamura|nm2077739|Suminojo Ichikawa|nm2373718|  Kitsuraku Arashi|\n",
            "|     48|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2373151|   Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     46|nm0648803|   Matsunosuke Onoe|nm1283907|      Utae Nakamura|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     45|nm1770187|Sen'nosuke Nakamura|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373718|  Kitsuraku Arashi|\n",
            "|     45|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2373151|    Chosei Kataoka|\n",
            "|     44|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2373151|   Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     42|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2373718| Kitsuraku Arashi|nm2426685|Kakumatsuro Arashi|\n",
            "|     42|nm2077739|  Suminojo Ichikawa|nm2082516|       Kijaku Ôtani|nm2373151|   Chosei Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     38|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2426685|Kakumatsuro Arashi|\n",
            "|     38|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2687024|     Rihaku Arashi|\n",
            "|     37|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2373718| Kitsuraku Arashi|nm2426685|Kakumatsuro Arashi|\n",
            "|     37|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2373718| Kitsuraku Arashi|nm2687024|     Rihaku Arashi|\n",
            "|     36|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2369538|  Ichitarô Kataoka|\n",
            "|     36|nm0648803|   Matsunosuke Onoe|nm2082516|       Kijaku Ôtani|nm2369538| Ichitarô Kataoka|nm2373718|  Kitsuraku Arashi|\n",
            "|     35|nm0297793|       Hideo Fujino|nm0455938| Teinosuke Kinugasa|nm0945427|  Kaichi Yamamoto|nm2394215|       Takeo Azuma|\n",
            "|     34|nm0648803|   Matsunosuke Onoe|nm2077739|  Suminojo Ichikawa|nm2082516|     Kijaku Ôtani|nm2414317|  Sentarô Nakamura|\n",
            "+-------+---------+-------------------+---------+-------------------+---------+-----------------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+------------------+---+---+\n",
            "|count|               avg|max|min|\n",
            "+-----+------------------+---+---+\n",
            "|   34|39.705882352941174| 86| 30|\n",
            "+-----+------------------+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:01:20.989749\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2021-07-20 11:38:06,264]:root: Calculating frequent itemsets for dataframe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+----+---+---+\n",
            "|count| avg|max|min|\n",
            "+-----+----+---+---+\n",
            "|    5|35.4| 44| 30|\n",
            "+-----+----+---+---+\n",
            "\n",
            "Execution time for dump_frequent_itemsets_stats: 0:01:30.737729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImdJLwTUsD7N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}